

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Artificial neural networks &#8212; ainotes</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'ml/ann';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Deep Learning" href="deep_learning.html" />
    <link rel="prev" title="Classification example: recognize handwritten digits" href="classification.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Preamble</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../about_ai.html">About Artificial Intelligence</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../foundations/computer_science.html">Computer science</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../foundations/programming.html">Programming</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../foundations/math.html">Mathematics</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../foundations/proba_stats.html">Probability &amp; statistics</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="supervised_learning.html">Supervised Learning</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="principles.html">Principles</a></li>
<li class="toctree-l2"><a class="reference internal" href="data.html">Data manipulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression.html">Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification.html">Classification</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Neural networks</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="deep_learning.html">Deep Learning</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="cnn.html">Convolutional neural networks</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="reinforcement_learning.html">Reinforcement Learning</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="rl.html">Introduction</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="challenges.html">Challenges</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="challenges/heart_disease.html">Heart disease</a></li>
<li class="toctree-l2"><a class="reference internal" href="challenges/cifar10.html">CIFAR10</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Decision-making</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../decision/introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../decision/confidence.html">Confidence</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Search algorithms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../search/dijkstra_astar.html">Dijkstra and A*</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../reference/acknowledgments.html">Acknowledgments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/bibliography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/bpesquet/ainotes/main?urlpath=tree/docs/ml/ann.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/bpesquet/ainotes/blob/main/docs/ml/ann.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/bpesquet/ainotes" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/bpesquet/ainotes/edit/main/docs/ml/ann.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/bpesquet/ainotes/issues/new?title=Issue%20on%20page%20%2Fml/ann.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/ml/ann.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Artificial neural networks</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#environment-setup">Environment setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fundamentals">Fundamentals</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#online-playground">Online playground</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#history">History</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-biological-inspiration">A biological inspiration</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mcculloch-pitts-formal-neuron-1943">McCulloch &amp; Pitts’ formal neuron (1943)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hebb-s-rule-1949">Hebb’s rule (1949)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#franck-rosenblatt-s-perceptron-1958">Franck Rosenblatt’s perceptron (1958)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-perceptron-learning-algorithm">The perceptron learning algorithm</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#minsky-s-critic-1969">Minsky’s critic (1969)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#decisive-breakthroughs-1970s-1990s">Decisive breakthroughs (1970s-1990s)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-components">Key components</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#anatomy-of-a-fully-connected-network">Anatomy of a fully connected network</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#neuron-output">Neuron output</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions">Activation functions</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid">Sigmoid</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#tanh">tanh</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#relu">ReLU</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-process">Training process</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-algorithm">Learning algorithm</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#weights-initialization">Weights initialization</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#weights-update">Weights update</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation">Backpropagation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#visual-demo-of-backpropagation">Visual demo of backpropagation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-classification-example">BInary classification example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-generation-and-visualization">Data generation and visualization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameters">Hyperparameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preparation">Data preparation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-definition">Model definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function">Loss function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-training">Model training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-results">Training results</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiclass-classification-example">Multiclass classification example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-loading-and-visualization">Data loading and visualization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Hyperparameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Data preparation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Model definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Loss function</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax">Softmax</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-algorithm">Optimization algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Model training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Training results</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="artificial-neural-networks">
<h1>Artificial neural networks<a class="headerlink" href="#artificial-neural-networks" title="Permalink to this heading">#</a></h1>
<section id="learning-objectives">
<h2>Learning objectives<a class="headerlink" href="#learning-objectives" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Know the possibilities, architecture and key components of an artificial neural network.</p></li>
<li><p>Understand how neural networks are trained.</p></li>
<li><p>Learn how to build neural networks with <a class="reference external" href="https://pytorch.org/">PyTorch</a>.</p></li>
</ul>
</section>
<section id="environment-setup">
<h2>Environment setup<a class="headerlink" href="#environment-setup" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">platform</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="kn">import</span> <span class="nn">sklearn</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_circles</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">torchvision.transforms</span> <span class="kn">import</span> <span class="n">ToTensor</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup plots</span>

<span class="c1"># Include matplotlib graphs into the notebook, next to the code</span>
<span class="c1"># https://stackoverflow.com/a/43028034/2380880</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="c1"># Improve plot quality</span>
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &quot;retina&quot;

<span class="c1"># Setup seaborn default theme</span>
<span class="c1"># http://seaborn.pydata.org/generated/seaborn.set_theme.html#seaborn.set_theme</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_theme</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Utility functions</span>


<span class="k">def</span> <span class="nf">plot_activation_function</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">f_prime</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Plot an activation function and its derivative&quot;&quot;&quot;</span>

    <span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">axis</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axis</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="s2">&quot;b-&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">f_prime</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="s2">&quot;g--&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">(x)&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">plot_dataset</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Plot a 2-dimensional dataset with associated classes&quot;&quot;&quot;</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;or&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;ob&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">plot_decision_boundary</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Plot the frontier between classes for a 2-dimensional dataset&quot;&quot;&quot;</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>

    <span class="c1"># Set min and max values and give it some padding</span>
    <span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mf">0.1</span>
    <span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mf">0.1</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mf">0.01</span>
    <span class="c1"># Generate a grid of points with distance h between them</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span>
    <span class="c1"># Compute model output for the whole grid</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="c1"># Convert PyTorch tensor to NumPy</span>
    <span class="n">zz</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="c1"># Plot the contour and training examples</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">zz</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">colormaps</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s2">&quot;Spectral&quot;</span><span class="p">))</span>
    <span class="n">cm_bright</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s2">&quot;#FF0000&quot;</span><span class="p">,</span> <span class="s2">&quot;#0000FF&quot;</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cm_bright</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">plot_loss_acc</span><span class="p">(</span><span class="n">history</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Plot training loss and accuracy. Takes a Keras-like History object as parameter&quot;&quot;&quot;</span>

    <span class="n">loss_values</span> <span class="o">=</span> <span class="n">history</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span>
    <span class="n">recorded_epochs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">loss_values</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">recorded_epochs</span><span class="p">,</span> <span class="n">loss_values</span><span class="p">,</span> <span class="s2">&quot;.--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training loss&quot;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="n">acc_values</span> <span class="o">=</span> <span class="n">history</span><span class="p">[</span><span class="s2">&quot;acc&quot;</span><span class="p">]</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">recorded_epochs</span><span class="p">,</span> <span class="n">acc_values</span><span class="p">,</span> <span class="s2">&quot;.--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training accuracy&quot;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Epochs&quot;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="n">final_loss</span> <span class="o">=</span> <span class="n">loss_values</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">final_acc</span> <span class="o">=</span> <span class="n">acc_values</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Training loss: </span><span class="si">{</span><span class="n">final_loss</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">. Training accuracy: </span><span class="si">{</span><span class="n">final_acc</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span>
    <span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">count_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return the total number of (trainable) parameters for a model&quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="p">(</span>
        <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">trainable</span>
        <span class="k">else</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">plot_fashion_images</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Plot some images with their associated labels&quot;&quot;&quot;</span>

    <span class="n">figure</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">cols</span><span class="p">,</span> <span class="n">rows</span> <span class="o">=</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">4</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">cols</span> <span class="o">*</span> <span class="n">rows</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">sample_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">img</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">sample_idx</span><span class="p">]</span>
        <span class="n">figure</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>

        <span class="c1"># Title is either true or predicted label</span>
        <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">title</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">label</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Add a dimension (to match expected shape with batch size) and store image on device memory</span>
            <span class="n">x_img</span> <span class="o">=</span> <span class="n">img</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="c1"># Compute predicted label for image</span>
            <span class="c1"># Even if the model outputs unormalized logits, argmax gives the predicted label</span>
            <span class="n">pred_label</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_img</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">title</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">labels</span><span class="p">[</span><span class="n">pred_label</span><span class="p">]</span><span class="si">}</span><span class="s2">?&quot;</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print environment info</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Python version: </span><span class="si">{</span><span class="n">platform</span><span class="o">.</span><span class="n">python_version</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;NumPy version: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;scikit-learn version: </span><span class="si">{</span><span class="n">sklearn</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;PyTorch version: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="c1"># PyTorch device configuration</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;CUDA GPU </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_name</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="si">}</span><span class="s2"> found :)&quot;</span><span class="p">)</span>
<span class="c1"># Performance issues exist with MPS backend</span>
<span class="c1"># elif torch.backends.mps.is_available():</span>
<span class="c1">#     device = torch.device(&quot;mps&quot;)</span>
<span class="c1">#     print(&quot;MPS GPU found :)&quot;)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;No GPU found, using CPU instead&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Python version: 3.11.1
NumPy version: 1.26.3
scikit-learn version: 1.3.2
PyTorch version: 2.0.1
No GPU found, using CPU instead
</pre></div>
</div>
</div>
</div>
</section>
<section id="fundamentals">
<h2>Fundamentals<a class="headerlink" href="#fundamentals" title="Permalink to this heading">#</a></h2>
<section id="online-playground">
<h3>Online playground<a class="headerlink" href="#online-playground" title="Permalink to this heading">#</a></h3>
<p><a class="reference external" href="http://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.1&amp;regularizationRate=0&amp;noise=0&amp;networkShape=2&amp;seed=0.59857&amp;showTestData=false&amp;discretize=false&amp;percTrainData=30&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false&amp;showTestData_hide=false&amp;problem_hide=true&amp;regularization_hide=true&amp;regularizationRate_hide=true&amp;percTrainData_hide=false"><img alt="TensorFlow playground" src="../_images/tf_playground.png" /></a></p>
</section>
<section id="history">
<h3>History<a class="headerlink" href="#history" title="Permalink to this heading">#</a></h3>
<section id="a-biological-inspiration">
<h4>A biological inspiration<a class="headerlink" href="#a-biological-inspiration" title="Permalink to this heading">#</a></h4>
<p><img alt="Neuron" src="../_images/neuron.png" /></p>
</section>
<section id="mcculloch-pitts-formal-neuron-1943">
<h4>McCulloch &amp; Pitts’ formal neuron (1943)<a class="headerlink" href="#mcculloch-pitts-formal-neuron-1943" title="Permalink to this heading">#</a></h4>
<p><img alt="Formal neuron model" src="../_images/neuron_model.jpeg" /></p>
</section>
<section id="hebb-s-rule-1949">
<h4>Hebb’s rule (1949)<a class="headerlink" href="#hebb-s-rule-1949" title="Permalink to this heading">#</a></h4>
<p>Attempt to explain synaptic plasticity, the adaptation of brain neurons during the learning process.</p>
<blockquote>
<div><p>“The general idea is an old one, that any two cells or systems of cells that are repeatedly active at the same time will tend to become ‘associated’ so that activity in one facilitates activity in the other.”</p>
</div></blockquote>
</section>
<section id="franck-rosenblatt-s-perceptron-1958">
<h4>Franck Rosenblatt’s perceptron (1958)<a class="headerlink" href="#franck-rosenblatt-s-perceptron-1958" title="Permalink to this heading">#</a></h4>
<p><img alt="The Perceptron" src="../_images/Perceptron.jpg" /></p>
</section>
<section id="the-perceptron-learning-algorithm">
<h4>The perceptron learning algorithm<a class="headerlink" href="#the-perceptron-learning-algorithm" title="Permalink to this heading">#</a></h4>
<ol class="arabic simple">
<li><p>Init randomly the connection weights <span class="math notranslate nohighlight">\(\pmb{\omega}\)</span>.</p></li>
<li><p>For each training sample <span class="math notranslate nohighlight">\(\pmb{x}^{(i)}\)</span>:</p>
<ol class="arabic simple">
<li><p>Compute the perceptron output <span class="math notranslate nohighlight">\(y'^{(i)}\)</span></p></li>
<li><p>Adjust weights : <span class="math notranslate nohighlight">\(\pmb{\omega_{t+1}} = \pmb{\omega_t} + \eta (y^{(i)} - y'^{(i)}) \pmb{x}^{(i)}\)</span></p></li>
</ol>
</li>
</ol>
</section>
<section id="minsky-s-critic-1969">
<h4>Minsky’s critic (1969)<a class="headerlink" href="#minsky-s-critic-1969" title="Permalink to this heading">#</a></h4>
<p>One perceptron cannot learn non-linearly separable functions.</p>
<p><img alt="XOR problem" src="../_images/xor.png" /></p>
<p>At the time, no learning algorithm existed for training the hidden layers of a MLP.</p>
</section>
<section id="decisive-breakthroughs-1970s-1990s">
<h4>Decisive breakthroughs (1970s-1990s)<a class="headerlink" href="#decisive-breakthroughs-1970s-1990s" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>1974: backpropagation theory (P. Werbos).</p></li>
<li><p>1986: learning through backpropagation (Rumelhart, Hinton, Williams).</p></li>
<li><p>1989: first researchs on deep neural nets (LeCun, Bengio).</p></li>
<li><p>1991: Universal approximation theorem. Given appropriate complexity and appropriate learning, a network can theorically approximate any continuous function.</p></li>
</ul>
</section>
</section>
<section id="key-components">
<h3>Key components<a class="headerlink" href="#key-components" title="Permalink to this heading">#</a></h3>
<section id="anatomy-of-a-fully-connected-network">
<h4>Anatomy of a fully connected network<a class="headerlink" href="#anatomy-of-a-fully-connected-network" title="Permalink to this heading">#</a></h4>
<p><img alt="A neural network" src="../_images/nn_weights.png" /></p>
</section>
<section id="neuron-output">
<h4>Neuron output<a class="headerlink" href="#neuron-output" title="Permalink to this heading">#</a></h4>
<p><img alt="Neuron output" src="../_images/neuron_output.png" /></p>
</section>
<section id="activation-functions">
<h4>Activation functions<a class="headerlink" href="#activation-functions" title="Permalink to this heading">#</a></h4>
<p>They are applied to the weighted sum of neuron inputs to produce its output.</p>
<p>They must be:</p>
<ul class="simple">
<li><p><strong>non-linear</strong>, so that the network has access to a richer representation space and not only linear transformations;</p></li>
<li><p><strong>differentiable</strong>, so that gradients can be computed during learning.</p></li>
</ul>
<section id="sigmoid">
<h5>Sigmoid<a class="headerlink" href="#sigmoid" title="Permalink to this heading">#</a></h5>
<p>This function “squashes” its input between between 0 and 1, outputting something that can be interpreted as the probability of the positive class. It is often used in the final layer of the network for binary classification tasks.</p>
<div class="math notranslate nohighlight">
\[\sigma(x) = \frac{1}{1 + e^{-x}}\]</div>
<div class="math notranslate nohighlight">
\[\sigma'(x) = \frac{e^{-x}}{(1 + e^{-x})^2} = \sigma(x)\big(1 - \sigma(x)\big)\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sigmoid function&quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">sigmoid_prime</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Derivative of the sigmoid function&quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_activation_function</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">,</span> <span class="n">sigmoid_prime</span><span class="p">,</span> <span class="s2">&quot;Sigmoid&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2d659848cba6dfdd88d723256d3fc76b5b94a4a1b5c0413b28f0772d013609e5.png" src="../_images/2d659848cba6dfdd88d723256d3fc76b5b94a4a1b5c0413b28f0772d013609e5.png" />
</div>
</div>
</section>
<section id="tanh">
<h5>tanh<a class="headerlink" href="#tanh" title="Permalink to this heading">#</a></h5>
<p>The hyperbolic tangent function has a similar shape as sigmoid, but outputs values in the <span class="math notranslate nohighlight">\([-1,1]\)</span> interval.</p>
<div class="math notranslate nohighlight">
\[tanh(x) = \frac{sinh(x)}{cosh(x)} = \frac{e^x - e^{-x}}{e^x + e^{-x}} = \frac{2}{1+e^{-2x}} -1 = 2\sigma(2x) - 1\]</div>
<div class="math notranslate nohighlight">
\[tanh'(x) = \frac{4}{(e^x + e^{-x})^2} = \frac{1}{cosh^2(x)}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Hyperbolic tangent function&quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">sigmoid</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>


<span class="k">def</span> <span class="nf">tanh_prime</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Derivative of hyperbolic tangent function&quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="mi">4</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_activation_function</span><span class="p">(</span><span class="n">tanh</span><span class="p">,</span> <span class="n">tanh_prime</span><span class="p">,</span> <span class="s2">&quot;Tanh&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7325af00f2ad3776c1c51ab5171a3f1c3dd5b378425aef1a1518a6d9b2ddb16f.png" src="../_images/7325af00f2ad3776c1c51ab5171a3f1c3dd5b378425aef1a1518a6d9b2ddb16f.png" />
</div>
</div>
</section>
<section id="relu">
<h5>ReLU<a class="headerlink" href="#relu" title="Permalink to this heading">#</a></h5>
<p>The Rectified Linear Unit function has replaced sigmoid and tanh as the default activation function in most contexts.</p>
<div class="math notranslate nohighlight">
\[ReLU(x) = max(0,x)\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}ReLU'(x) =
    \begin{cases}
      0 \; \forall x \in\; ]-\infty, 0] \\
      1 \; \forall x \in\; ]0, +\infty[
    \end{cases}\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Rectified Linear Unit function&quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">relu_prime</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Derivative of the Rectified Linear Unit function&quot;&quot;&quot;</span>

    <span class="c1"># https://stackoverflow.com/a/45022037</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_activation_function</span><span class="p">(</span><span class="n">relu</span><span class="p">,</span> <span class="n">relu_prime</span><span class="p">,</span> <span class="s2">&quot;ReLU&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">6.1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7a4da6b25a7527b41e08c76d07c232f6898b265e40db8802748843ec9ab8b741.png" src="../_images/7a4da6b25a7527b41e08c76d07c232f6898b265e40db8802748843ec9ab8b741.png" />
</div>
</div>
</section>
</section>
</section>
<section id="training-process">
<h3>Training process<a class="headerlink" href="#training-process" title="Permalink to this heading">#</a></h3>
<section id="learning-algorithm">
<h4>Learning algorithm<a class="headerlink" href="#learning-algorithm" title="Permalink to this heading">#</a></h4>
<p><a class="reference external" href="https://www.manning.com/books/deep-learning-with-python"><img alt="Extract from the book Deep Learning with Python" src="../_images/nn_learning.jpg" /></a></p>
</section>
<section id="weights-initialization">
<h4>Weights initialization<a class="headerlink" href="#weights-initialization" title="Permalink to this heading">#</a></h4>
<p>To facilitate training, initial weights must be:</p>
<ul class="simple">
<li><p>non-zero</p></li>
<li><p>random</p></li>
<li><p>have small values</p></li>
</ul>
<p><a class="reference external" href="https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79">Several techniques exist</a>. A commonly used one is <a class="reference external" href="https://proceedings.mlr.press/v9/glorot10a.html">Xavier initialization</a>.</p>
</section>
<section id="weights-update">
<h4>Weights update<a class="headerlink" href="#weights-update" title="Permalink to this heading">#</a></h4>
<p>Objective: minimize the loss function. Method: <a class="reference internal" href="principles.html"><span class="doc std std-doc">gradient descent</span></a>.</p>
<div class="math notranslate nohighlight">
\[\pmb{\omega_{t+1}} = \pmb{\omega_t} - \eta\nabla_{\pmb{\omega}}\mathcal{L}(\pmb{\omega_t})\]</div>
</section>
<section id="backpropagation">
<h4>Backpropagation<a class="headerlink" href="#backpropagation" title="Permalink to this heading">#</a></h4>
<p>Objective: compute <span class="math notranslate nohighlight">\(\nabla_{\pmb{\omega}}\mathcal{L}(\pmb{\omega_t})\)</span>, the loss function gradient w.r.t. all the network weights.</p>
<p>Method: apply the <a class="reference external" href="https://en.wikipedia.org/wiki/Chain_rule">chain rule</a> to compute partial derivatives backwards, starting from the current output.</p>
<div class="math notranslate nohighlight">
\[y = f(g(x)) \;\;\;\; \frac{\partial y}{\partial x} = \frac{\partial f}{\partial g} \frac{\partial g}{\partial x}\;\;\;\; \frac{\partial y}{\partial x} = \sum_{i=1}^n \frac{\partial f}{\partial g^{(i)}} \frac{\partial g^{(i)}}{\partial x}\]</div>
</section>
<section id="visual-demo-of-backpropagation">
<h4>Visual demo of backpropagation<a class="headerlink" href="#visual-demo-of-backpropagation" title="Permalink to this heading">#</a></h4>
<p><a class="reference external" href="https://developers-dot-devsite-v2-prod.appspot.com/machine-learning/crash-course/backprop-scroll"><img alt="Backprop explained visually" src="../_images/visual_backprop_demo.png" /></a></p>
</section>
</section>
</section>
<section id="binary-classification-example">
<h2>BInary classification example<a class="headerlink" href="#binary-classification-example" title="Permalink to this heading">#</a></h2>
<section id="data-generation-and-visualization">
<h3>Data generation and visualization<a class="headerlink" href="#data-generation-and-visualization" title="Permalink to this heading">#</a></h3>
<p>A scikit-learn <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html">function</a> is used to easily generate two-dimensional data with two classes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate 2D data (a large circle containing a smaller circle)</span>
<span class="n">planar_data</span><span class="p">,</span> <span class="n">planar_targets</span> <span class="o">=</span> <span class="n">make_circles</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Data: </span><span class="si">{</span><span class="n">planar_data</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">. targets: </span><span class="si">{</span><span class="n">planar_targets</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">planar_data</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">planar_targets</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Data: (500, 2). targets: (500,)
[[-0.32467672 -0.29603112]
 [ 0.40003061  0.00660986]
 [ 0.08621761 -0.40238069]
 [-0.00974224  0.18402813]
 [ 0.74864346 -0.79439671]
 [ 0.55674722  0.68585686]
 [ 0.35345507  0.10892971]
 [ 0.16709313 -0.80245418]
 [-0.38029537 -0.27048706]
 [-0.4123724  -0.95964059]]
[1 1 1 1 0 0 1 0 1 0]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_dataset</span><span class="p">(</span><span class="n">planar_data</span><span class="p">,</span> <span class="n">planar_targets</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/126e161ae875d812d43caced9325fed514d8e7022c23db54c0a2458e0c60fe3c.png" src="../_images/126e161ae875d812d43caced9325fed514d8e7022c23db54c0a2458e0c60fe3c.png" />
</div>
</div>
</section>
<section id="hyperparameters">
<h3>Hyperparameters<a class="headerlink" href="#hyperparameters" title="Permalink to this heading">#</a></h3>
<p>Hyperparameters (<span class="math notranslate nohighlight">\(\neq\)</span> model parameters) are adjustable configuration values that let you control the model training process.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Rate of parameter change during gradient descent</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># An epoch is finished when all data samples have been presented to the model during training</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">50</span>

<span class="c1"># Number of samples used for one gradient descent step during training</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">5</span>

<span class="c1"># Number of neurons on the hidden layer of the MLP</span>
<span class="n">hidden_layer_size</span> <span class="o">=</span> <span class="mi">2</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="data-preparation">
<h3>Data preparation<a class="headerlink" href="#data-preparation" title="Permalink to this heading">#</a></h3>
<p>Generated data (NumPy tensors) needs to be converted to PyTorch tensors before training a PyTorch-based model. These new tensors are stored in the memory of the available device (GPU ou CPU).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create PyTorch tensors from NumPy tensors</span>

<span class="n">x_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">planar_data</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># PyTorch loss function expects float results of shape (batch_size, 1) instead of (batch_size,)</span>
<span class="c1"># So we add a new axis and convert them to floats</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">planar_targets</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;x_train: </span><span class="si">{</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">. y_train: </span><span class="si">{</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>x_train: torch.Size([500, 2]). y_train: torch.Size([500, 1])
</pre></div>
</div>
</div>
</div>
<p>In order to use <a class="reference internal" href="principles.html"><span class="doc std std-doc">mini-batch SGD</span></a>, data needs to be passed to the model as small, randomized batches during training. The Pytorch <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">DataLoader</a> class abstracts this complexity for us.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load data as randomized batches for training</span>
<span class="n">planar_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)),</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="model-definition">
<h3>Model definition<a class="headerlink" href="#model-definition" title="Permalink to this heading">#</a></h3>
<p>A PyTorch model is defined by combining elementary blocks, known as <em>modules</em>.</p>
<p>Our neural network uses the following ones:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html">Sequential</a>: an ordered container of modules.</p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html">Linear</a>: a linear transformation of its entries, a.k.a. <em>dense</em> or <em>fully connected</em> layer.</p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html">Tanh</a> and <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html">Sigmoid</a>: the corresponding activation functions.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a MultiLayer Perceptron with 2 inputs and 1 output</span>
<span class="c1"># You may change its internal architecture:</span>
<span class="c1"># for example, try adding one neuron on the hidden layer and check training results</span>
<span class="n">planar_model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="c1"># Hidden layer with 2 inputs</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">hidden_layer_size</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
    <span class="c1"># Output layer</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">hidden_layer_size</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">planar_model</span><span class="p">)</span>

<span class="c1"># Count the total number of trainable model parameters (weights)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of trainable parameters: </span><span class="si">{</span><span class="n">count_parameters</span><span class="p">(</span><span class="n">planar_model</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Tanh()
  (2): Linear(in_features=2, out_features=1, bias=True)
  (3): Sigmoid()
)
Number of trainable parameters: 9.
</pre></div>
</div>
</div>
</div>
</section>
<section id="loss-function">
<h3>Loss function<a class="headerlink" href="#loss-function" title="Permalink to this heading">#</a></h3>
<p>For binary classification tasks, the standard choice is the <a class="reference internal" href="classification.html"><span class="doc std std-doc">binary cross entropy loss</span></a>, conveniently provided by a <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html">PyTorch class</a>.</p>
<p>For each sample of the batch, it will compare the output of the model (a value <span class="math notranslate nohighlight">\(\in [0,1]\)</span> provided by the sigmoid function) with the expected binary value <span class="math notranslate nohighlight">\(\in \{0,1\}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Binary cross entropy loss function</span>
<span class="n">planar_loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="model-training">
<h3>Model training<a class="headerlink" href="#model-training" title="Permalink to this heading">#</a></h3>
<p>The training algorithm is as follows:</p>
<ul class="simple">
<li><p>On each iteration on the whole dataset (known as an <em>epoch</em>) and for each data batch inside an epoch, the model output is computed on the current batch.</p></li>
<li><p>This output is used alongside expected results by the loss function to obtain the mean loss for the current batch.</p></li>
<li><p>The gradient of the loss w.r.t. each model parameter is computed (backpropagation).</p></li>
<li><p>The model parameters are updated in the opposite direction of their gradient (one GD step).</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_planar</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Main training code&quot;&quot;&quot;</span>

    <span class="c1"># Object storing training history</span>
    <span class="n">history</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="p">[],</span> <span class="s2">&quot;acc&quot;</span><span class="p">:</span> <span class="p">[]}</span>

    <span class="c1"># Number of samples</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>

    <span class="c1"># Number of batches in an epoch (= n_samples / batch_size, rounded up)</span>
    <span class="n">n_batches</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training started! </span><span class="si">{</span><span class="n">n_samples</span><span class="si">}</span><span class="s2"> samples. </span><span class="si">{</span><span class="n">n_batches</span><span class="si">}</span><span class="s2"> batches per epoch&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="c1"># Reset total loss for the current epoch</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Reset number of correct predictions for the current epoch</span>
        <span class="n">n_correct</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Training loop for one data batch (i.e. one gradient descent step)</span>
        <span class="k">for</span> <span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
            <span class="c1"># Zero the gradients before running the backward pass</span>
            <span class="c1"># Avoids accumulating gradients erroneously</span>
            <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

            <span class="c1"># Forward pass: compute model output with current weights</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_batch</span><span class="p">)</span>

            <span class="c1"># Compute loss (comparison between expected and actual results)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">)</span>

            <span class="c1"># Backward pass (backprop): compute gradient of the loss w.r.t each model weight</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

            <span class="c1"># Gradient descent step: update the weights in the opposite direction of their gradient</span>
            <span class="c1"># no_grad() avoids tracking operations history, which would be useless here</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
                    <span class="n">param</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span>

                <span class="c1"># Accumulate data for epoch metrics: loss and number of correct predictions</span>
                <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="n">n_correct</span> <span class="o">+=</span> <span class="p">(</span>
                    <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">x_batch</span><span class="p">))</span> <span class="o">==</span> <span class="n">y_batch</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="p">)</span>

        <span class="c1"># Compute epoch metrics</span>
        <span class="n">epoch_loss</span> <span class="o">=</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="n">n_batches</span>
        <span class="n">epoch_acc</span> <span class="o">=</span> <span class="n">n_correct</span> <span class="o">/</span> <span class="n">n_samples</span>

        <span class="nb">print</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Epoch [</span><span class="si">{</span><span class="p">(</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="si">:</span><span class="s2">3</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">epochs</span><span class="si">:</span><span class="s2">3</span><span class="si">}</span><span class="s2">]. Mean loss: </span><span class="si">{</span><span class="n">epoch_loss</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">. Accuracy: </span><span class="si">{</span><span class="n">epoch_acc</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span>
        <span class="p">)</span>

        <span class="c1"># Record epoch metrics for later plotting</span>
        <span class="n">history</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch_loss</span><span class="p">)</span>
        <span class="n">history</span><span class="p">[</span><span class="s2">&quot;acc&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch_acc</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training complete! Total gradient descent steps: </span><span class="si">{</span><span class="n">epochs</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">n_batches</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">history</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">planar_history</span> <span class="o">=</span> <span class="n">train_planar</span><span class="p">(</span>
    <span class="n">dataloader</span><span class="o">=</span><span class="n">planar_dataloader</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">planar_model</span><span class="p">,</span>
    <span class="n">loss_fn</span><span class="o">=</span><span class="n">planar_loss_fn</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training started! 500 samples. 100 batches per epoch
Epoch [  1/ 50]. Mean loss: 0.70262. Accuracy: 53.00%
Epoch [  2/ 50]. Mean loss: 0.69675. Accuracy: 51.60%
Epoch [  3/ 50]. Mean loss: 0.69314. Accuracy: 61.60%
Epoch [  4/ 50]. Mean loss: 0.68770. Accuracy: 59.40%
Epoch [  5/ 50]. Mean loss: 0.68144. Accuracy: 59.80%
Epoch [  6/ 50]. Mean loss: 0.67122. Accuracy: 59.40%
Epoch [  7/ 50]. Mean loss: 0.65305. Accuracy: 58.20%
Epoch [  8/ 50]. Mean loss: 0.63543. Accuracy: 63.00%
Epoch [  9/ 50]. Mean loss: 0.61463. Accuracy: 67.40%
Epoch [ 10/ 50]. Mean loss: 0.58134. Accuracy: 75.40%
Epoch [ 11/ 50]. Mean loss: 0.53992. Accuracy: 82.20%
Epoch [ 12/ 50]. Mean loss: 0.49717. Accuracy: 83.40%
Epoch [ 13/ 50]. Mean loss: 0.46294. Accuracy: 85.40%
Epoch [ 14/ 50]. Mean loss: 0.44274. Accuracy: 84.60%
Epoch [ 15/ 50]. Mean loss: 0.42935. Accuracy: 85.40%
Epoch [ 16/ 50]. Mean loss: 0.41580. Accuracy: 85.40%
Epoch [ 17/ 50]. Mean loss: 0.41020. Accuracy: 85.60%
Epoch [ 18/ 50]. Mean loss: 0.40576. Accuracy: 84.60%
Epoch [ 19/ 50]. Mean loss: 0.39901. Accuracy: 85.00%
Epoch [ 20/ 50]. Mean loss: 0.40043. Accuracy: 84.80%
Epoch [ 21/ 50]. Mean loss: 0.39271. Accuracy: 85.40%
Epoch [ 22/ 50]. Mean loss: 0.39377. Accuracy: 85.60%
Epoch [ 23/ 50]. Mean loss: 0.39023. Accuracy: 85.00%
Epoch [ 24/ 50]. Mean loss: 0.38722. Accuracy: 85.00%
Epoch [ 25/ 50]. Mean loss: 0.38693. Accuracy: 86.00%
Epoch [ 26/ 50]. Mean loss: 0.38385. Accuracy: 85.40%
Epoch [ 27/ 50]. Mean loss: 0.38378. Accuracy: 85.40%
Epoch [ 28/ 50]. Mean loss: 0.38171. Accuracy: 84.80%
Epoch [ 29/ 50]. Mean loss: 0.37918. Accuracy: 85.40%
Epoch [ 30/ 50]. Mean loss: 0.38222. Accuracy: 84.80%
Epoch [ 31/ 50]. Mean loss: 0.38052. Accuracy: 85.60%
Epoch [ 32/ 50]. Mean loss: 0.37954. Accuracy: 85.40%
Epoch [ 33/ 50]. Mean loss: 0.37514. Accuracy: 85.20%
Epoch [ 34/ 50]. Mean loss: 0.37813. Accuracy: 85.60%
Epoch [ 35/ 50]. Mean loss: 0.37574. Accuracy: 84.60%
Epoch [ 36/ 50]. Mean loss: 0.37466. Accuracy: 85.40%
Epoch [ 37/ 50]. Mean loss: 0.37679. Accuracy: 85.60%
Epoch [ 38/ 50]. Mean loss: 0.37439. Accuracy: 85.00%
Epoch [ 39/ 50]. Mean loss: 0.37387. Accuracy: 85.20%
Epoch [ 40/ 50]. Mean loss: 0.37079. Accuracy: 84.80%
Epoch [ 41/ 50]. Mean loss: 0.37035. Accuracy: 85.80%
Epoch [ 42/ 50]. Mean loss: 0.37122. Accuracy: 85.20%
Epoch [ 43/ 50]. Mean loss: 0.37228. Accuracy: 85.40%
Epoch [ 44/ 50]. Mean loss: 0.37338. Accuracy: 85.20%
Epoch [ 45/ 50]. Mean loss: 0.37170. Accuracy: 85.60%
Epoch [ 46/ 50]. Mean loss: 0.37078. Accuracy: 84.80%
Epoch [ 47/ 50]. Mean loss: 0.37189. Accuracy: 85.40%
Epoch [ 48/ 50]. Mean loss: 0.37002. Accuracy: 84.40%
Epoch [ 49/ 50]. Mean loss: 0.36965. Accuracy: 85.40%
Epoch [ 50/ 50]. Mean loss: 0.36996. Accuracy: 85.40%
Training complete! Total gradient descent steps: 5000
</pre></div>
</div>
</div>
</div>
</section>
<section id="training-results">
<h3>Training results<a class="headerlink" href="#training-results" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_loss_acc</span><span class="p">(</span><span class="n">planar_history</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/9d8995e577f9a3f335a6e7f4107b86ac6b7d9671018669e96bbbb6bdc746103d.png" src="../_images/9d8995e577f9a3f335a6e7f4107b86ac6b7d9671018669e96bbbb6bdc746103d.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_decision_boundary</span><span class="p">(</span><span class="n">planar_model</span><span class="p">,</span> <span class="n">planar_data</span><span class="p">,</span> <span class="n">planar_targets</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/93601e68ba84819cefd91e649362615dece2b09ee13d96c9852601abd5888add.png" src="../_images/93601e68ba84819cefd91e649362615dece2b09ee13d96c9852601abd5888add.png" />
</div>
</div>
</section>
</section>
<section id="multiclass-classification-example">
<h2>Multiclass classification example<a class="headerlink" href="#multiclass-classification-example" title="Permalink to this heading">#</a></h2>
<section id="data-loading-and-visualization">
<h3>Data loading and visualization<a class="headerlink" href="#data-loading-and-visualization" title="Permalink to this heading">#</a></h3>
<p>We use the <a class="reference external" href="https://github.com/zalandoresearch/fashion-mnist">Fashion-MNIST</a> dataset, analogous to the famous MNIST handwritten digits dataset. It consists of:</p>
<ul class="simple">
<li><p>a training set containing 60,000 28x28 grayscale images, each of them associated with a label (fashion category) from 10 classes;</p></li>
<li><p>a test set of 10,000 images with the same properties.</p></li>
</ul>
<p>A <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html">PyTorch class</a> simplifies the loading process of this dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fashion_train_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span>
    <span class="n">root</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">ToTensor</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">fashion_test_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span>
    <span class="n">root</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">ToTensor</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Show info about the first training image</span>
<span class="n">fashion_img</span><span class="p">,</span> <span class="n">fashion_label</span> <span class="o">=</span> <span class="n">fashion_train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;First image: </span><span class="si">{</span><span class="n">fashion_img</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">. Label: </span><span class="si">{</span><span class="n">fashion_label</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>First image: torch.Size([1, 28, 28]). Label: 9
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Show raw data for the first image</span>
<span class="c1"># Pixel values have already been normalized into the [0,1] range</span>
<span class="nb">print</span><span class="p">(</span><span class="n">fashion_img</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,
          0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0039, 0.0039, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,
          0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,
          0.0157, 0.0000, 0.0000, 0.0118],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,
          0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0471, 0.0392, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,
          0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,
          0.3020, 0.5098, 0.2824, 0.0588],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,
          0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,
          0.5529, 0.3451, 0.6745, 0.2588],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,
          0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,
          0.4824, 0.7686, 0.8980, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,
          0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,
          0.8745, 0.9608, 0.6784, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,
          0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,
          0.8627, 0.9529, 0.7922, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,
          0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,
          0.8863, 0.7725, 0.8196, 0.2039],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,
          0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,
          0.9608, 0.4667, 0.6549, 0.2196],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,
          0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,
          0.8510, 0.8196, 0.3608, 0.0000],
         [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,
          0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,
          0.8549, 1.0000, 0.3020, 0.0000],
         [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,
          0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,
          0.8784, 0.9569, 0.6235, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,
          0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,
          0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,
          0.9137, 0.9333, 0.8431, 0.0000],
         [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,
          0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,
          0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,
          0.8627, 0.9098, 0.9647, 0.0000],
         [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,
          0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,
          0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,
          0.8706, 0.8941, 0.8824, 0.0000],
         [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,
          0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,
          0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,
          0.8745, 0.8784, 0.8980, 0.1137],
         [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,
          0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,
          0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,
          0.8627, 0.8667, 0.9020, 0.2627],
         [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,
          0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,
          0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,
          0.7098, 0.8039, 0.8078, 0.4510],
         [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,
          0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,
          0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,
          0.6549, 0.6941, 0.8235, 0.3608],
         [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,
          0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,
          0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,
          0.7529, 0.8471, 0.6667, 0.0000],
         [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,
          0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,
          0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,
          0.3882, 0.2275, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,
          0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000]]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Labels, i.e. fashion categories associated to images (one category per image)</span>
<span class="n">fashion_labels</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">&quot;T-Shirt&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Trouser&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Pullover&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Dress&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Coat&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Sandal&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Shirt&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Sneaker&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Bag&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Ankle Boot&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_fashion_images</span><span class="p">(</span><span class="n">fashion_train_data</span><span class="p">,</span> <span class="n">fashion_labels</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/716bd5cc2367835ef3f316653b9dca9120c08be2bc4c2d4d0da2b40d06d86b1b.png" src="../_images/716bd5cc2367835ef3f316653b9dca9120c08be2bc4c2d4d0da2b40d06d86b1b.png" />
</div>
</div>
</section>
<section id="id1">
<h3>Hyperparameters<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Try to change the learning rate to 1e-2 ans check training results</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id2">
<h3>Data preparation<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<p>As always, data will be passed to the model as small, randomized batches during training.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fashion_train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">fashion_train_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">fashion_test_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">fashion_test_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id3">
<h3>Model definition<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h3>
<p>Most PyTorch models are defined as subclasses of the <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html">Module</a> class. Their constructor creates the layer architecture and their <code class="docutils literal notranslate"><span class="pre">forward</span></code> method defines the forward pass of the model.</p>
<p>In this model, we use the <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html">Flatten</a> module that transforms an input tensor of any shape into a vector (hence its name).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Neural network for fashion articles classification&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Flatten the input image of shape (1, 28, 28) into a vector of shape (28*28,)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>

        <span class="c1"># Define a sequential stack of linear layers and activation functions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_stack</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="c1"># First hidden layer with 784 inputs</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">64</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="c1"># Second hidden layer</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">64</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="c1"># Output layer</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Define the forward pass of the model&quot;&quot;&quot;</span>

        <span class="c1"># Apply flattening to input</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Compute output of layer stack</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_stack</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Logits are a vector of raw (non-normalized) predictions</span>
        <span class="c1"># This vector contains 10 values, one for each possible class</span>
        <span class="k">return</span> <span class="n">logits</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fashion_model</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">fashion_model</span><span class="p">)</span>

<span class="c1"># Try to guess the total number of parameters for this model before running this code!</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of trainable parameters: </span><span class="si">{</span><span class="n">count_parameters</span><span class="p">(</span><span class="n">fashion_model</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>NeuralNetwork(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (layer_stack): Sequential(
    (0): Linear(in_features=784, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=10, bias=True)
  )
)
Number of trainable parameters: 55050
</pre></div>
</div>
</div>
</div>
</section>
<section id="id4">
<h3>Loss function<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h3>
<p>The standard choice for multiclass classification tasks is the <a class="reference internal" href="classification.html"><span class="doc std std-doc">cross entropy loss</span></a> a.k.a. negative log-likelihood loss, provided by a PyTorch class aptly named <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html">CrossEntropyLoss</a>.</p>
<blockquote>
<div><p>PyTorch also offers the <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss">NLLLoss</a> class implementing the negative log-likelihood loss. A key difference is that <code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code> expects <em>logits</em>  (raw, unnormalized predictions) as inputs, and uses <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax">LogSoftmax</a> to transform them into probabilities before computing its output. Using <code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code> is equivalent to applying <code class="docutils literal notranslate"><span class="pre">LogSoftmax</span></code> followed by <code class="docutils literal notranslate"><span class="pre">NLLLoss</span></code> (<a class="reference external" href="https://towardsdatascience.com/cross-entropy-negative-log-likelihood-and-all-that-jazz-47a95bd2e81">more details</a>).</p>
</div></blockquote>
<section id="softmax">
<h4>Softmax<a class="headerlink" href="#softmax" title="Permalink to this heading">#</a></h4>
<p>The softmax function turns a vector <span class="math notranslate nohighlight">\(\pmb{v} = \{v_1, v_2, \dots, v_K \} \in \mathbb{R}^K\)</span> of raws values (called a <em>logits vector</em> when it’s the output of a ML model) into a probability distribution. It is a multiclass generalization of the sigmoid function.</p>
<div class="math notranslate nohighlight">
\[\sigma(\pmb{v})_k = \frac{e^{v_k}}{\sum_{k=1}^K {e^{v_k}}}\;\;\;\;
\sum_{k=1}^K \sigma(\pmb{v})_k = 1\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(K\)</span>: number of labels.</p></li>
<li><p><span class="math notranslate nohighlight">\(\pmb{v}\)</span>: logits vector, i.e. raw predictions for each class.</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma(\pmb{v})_k \in [0,1]\)</span>: probability associated to label <span class="math notranslate nohighlight">\(k \in [1,K]\)</span>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Softmax function&quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>


<span class="c1"># Raw values (logits)</span>
<span class="n">raw_predictions</span> <span class="o">=</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]</span>

<span class="n">probas</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">raw_predictions</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">probas</span><span class="p">)</span>

<span class="c1"># Sum of all probabilities is equal to 1</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">probas</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.8360188  0.11314284 0.05083836]
0.9999999999999999
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="optimization-algorithm">
<h3>Optimization algorithm<a class="headerlink" href="#optimization-algorithm" title="Permalink to this heading">#</a></h3>
<p>PyTorch provides out-of-the-box implementations for many gradient descent optimization algorithms (<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html">Adam</a>, <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html">RMSProp</a>, etc).</p>
<p>We’ll stick with vanilla mini-batch SGD for now.</p>
</section>
<section id="id5">
<h3>Model training<a class="headerlink" href="#id5" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">epoch_loop</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Training algorithm for one epoch&quot;&quot;&quot;</span>

    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">n_correct</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
        <span class="c1"># Load data and targets on device memory</span>
        <span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="o">=</span> <span class="n">x_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Reset gradients</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># Forward pass</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_batch</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">)</span>

        <span class="c1"># Backward pass: backprop and GD step</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="c1"># Accumulate data for epoch metrics: loss and number of correct predictions</span>
            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">n_correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">x_batch</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y_batch</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">total_loss</span><span class="p">,</span> <span class="n">n_correct</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Main training code&quot;&quot;&quot;</span>

    <span class="n">history</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="p">[],</span> <span class="s2">&quot;acc&quot;</span><span class="p">:</span> <span class="p">[]}</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
    <span class="n">n_batches</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training started! </span><span class="si">{</span><span class="n">n_samples</span><span class="si">}</span><span class="s2"> samples. </span><span class="si">{</span><span class="n">n_batches</span><span class="si">}</span><span class="s2"> batches per epoch&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">total_loss</span><span class="p">,</span> <span class="n">n_correct</span> <span class="o">=</span> <span class="n">epoch_loop</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>

        <span class="c1"># Compute epoch metrics</span>
        <span class="n">epoch_loss</span> <span class="o">=</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="n">n_batches</span>
        <span class="n">epoch_acc</span> <span class="o">=</span> <span class="n">n_correct</span> <span class="o">/</span> <span class="n">n_samples</span>

        <span class="nb">print</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Epoch [</span><span class="si">{</span><span class="p">(</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="si">:</span><span class="s2">3</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">epochs</span><span class="si">:</span><span class="s2">3</span><span class="si">}</span><span class="s2">]. Mean loss: </span><span class="si">{</span><span class="n">epoch_loss</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">. Accuracy: </span><span class="si">{</span><span class="n">epoch_acc</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span>
        <span class="p">)</span>

        <span class="c1"># Record epoch metrics for later plotting</span>
        <span class="n">history</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch_loss</span><span class="p">)</span>
        <span class="n">history</span><span class="p">[</span><span class="s2">&quot;acc&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch_acc</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training complete! Total gradient descent steps: </span><span class="si">{</span><span class="n">epochs</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">n_batches</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">history</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fashion_history</span> <span class="o">=</span> <span class="n">fit</span><span class="p">(</span>
    <span class="n">dataloader</span><span class="o">=</span><span class="n">fashion_train_dataloader</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">fashion_model</span><span class="p">,</span>
    <span class="n">loss_fn</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(),</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">fashion_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">),</span>
    <span class="n">epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training started! 60000 samples. 938 batches per epoch
Epoch [  1/ 10]. Mean loss: 2.27095. Accuracy: 17.39%
Epoch [  2/ 10]. Mean loss: 2.14185. Accuracy: 39.14%
Epoch [  3/ 10]. Mean loss: 1.88409. Accuracy: 42.12%
Epoch [  4/ 10]. Mean loss: 1.57735. Accuracy: 50.45%
Epoch [  5/ 10]. Mean loss: 1.33551. Accuracy: 60.20%
Epoch [  6/ 10]. Mean loss: 1.16612. Accuracy: 63.36%
Epoch [  7/ 10]. Mean loss: 1.04686. Accuracy: 65.08%
Epoch [  8/ 10]. Mean loss: 0.96125. Accuracy: 66.36%
Epoch [  9/ 10]. Mean loss: 0.89909. Accuracy: 67.62%
Epoch [ 10/ 10]. Mean loss: 0.85267. Accuracy: 68.94%
Training complete! Total gradient descent steps: 9380
</pre></div>
</div>
</div>
</div>
</section>
<section id="id6">
<h3>Training results<a class="headerlink" href="#id6" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_loss_acc</span><span class="p">(</span><span class="n">fashion_history</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/895703911de1518b57e9dc9d0c74b04630c84f6ab98fbc3977e571297f9f86bb.png" src="../_images/895703911de1518b57e9dc9d0c74b04630c84f6ab98fbc3977e571297f9f86bb.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_fashion_images</span><span class="p">(</span><span class="n">fashion_train_data</span><span class="p">,</span> <span class="n">fashion_labels</span><span class="p">,</span> <span class="n">fashion_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/e69a6953bc9a500e45a4f8ccec83ba04566f82f171a86987790bb74785c65d93.png" src="../_images/e69a6953bc9a500e45a4f8ccec83ba04566f82f171a86987790bb74785c65d93.png" />
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./ml"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="classification.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Classification example: recognize handwritten digits</p>
      </div>
    </a>
    <a class="right-next"
       href="deep_learning.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Deep Learning</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#environment-setup">Environment setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fundamentals">Fundamentals</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#online-playground">Online playground</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#history">History</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-biological-inspiration">A biological inspiration</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mcculloch-pitts-formal-neuron-1943">McCulloch &amp; Pitts’ formal neuron (1943)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hebb-s-rule-1949">Hebb’s rule (1949)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#franck-rosenblatt-s-perceptron-1958">Franck Rosenblatt’s perceptron (1958)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-perceptron-learning-algorithm">The perceptron learning algorithm</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#minsky-s-critic-1969">Minsky’s critic (1969)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#decisive-breakthroughs-1970s-1990s">Decisive breakthroughs (1970s-1990s)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-components">Key components</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#anatomy-of-a-fully-connected-network">Anatomy of a fully connected network</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#neuron-output">Neuron output</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions">Activation functions</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid">Sigmoid</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#tanh">tanh</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#relu">ReLU</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-process">Training process</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-algorithm">Learning algorithm</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#weights-initialization">Weights initialization</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#weights-update">Weights update</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation">Backpropagation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#visual-demo-of-backpropagation">Visual demo of backpropagation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-classification-example">BInary classification example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-generation-and-visualization">Data generation and visualization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameters">Hyperparameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preparation">Data preparation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-definition">Model definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function">Loss function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-training">Model training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-results">Training results</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiclass-classification-example">Multiclass classification example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-loading-and-visualization">Data loading and visualization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Hyperparameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Data preparation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Model definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Loss function</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax">Softmax</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-algorithm">Optimization algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Model training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Training results</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Baptiste Pesquet
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023-present.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>