
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Introduction to Reinforcement Learning &#8212; ainotes</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'ml/rl';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Challenges" href="challenges.html" />
    <link rel="prev" title="Reinforcement Learning" href="reinforcement_learning.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="ainotes - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="ainotes - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Preamble</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../about_ai.html">About Artificial Intelligence</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../foundations/computer_science.html">Computer science</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../foundations/programming.html">Programming</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../foundations/math.html">Mathematics</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../foundations/proba_stats.html">Probability &amp; statistics</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="supervised_learning.html">Supervised Learning</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="principles.html">Principles</a></li>
<li class="toctree-l2"><a class="reference internal" href="data.html">Data manipulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression.html">Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification.html">Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="ann.html">Neural networks</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="deep_learning.html">Deep Learning</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="cnn.html">Convolutional neural networks</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="reinforcement_learning.html">Reinforcement Learning</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Introduction</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="challenges.html">Challenges</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="challenges/heart_disease.html">Heart disease</a></li>
<li class="toctree-l2"><a class="reference internal" href="challenges/cifar10.html">CIFAR10</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Decision-making</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../decision/introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../decision/confidence.html">Confidence</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Search algorithms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../search/dijkstra_astar.html">Dijkstra and A*</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../reference/acknowledgments.html">Acknowledgments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/bibliography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/bpesquet/ainotes/main?urlpath=tree/docs/ml/rl.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/bpesquet/ainotes/blob/main/docs/ml/rl.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/bpesquet/ainotes" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/bpesquet/ainotes/edit/main/docs/ml/rl.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/bpesquet/ainotes/issues/new?title=Issue%20on%20page%20%2Fml/rl.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/ml/rl.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Introduction to Reinforcement Learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#environnement-setup">Environnement setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-reinforcement-learning">What is Reinforcement Learning?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rl-in-a-nutshell">RL in a nutshell</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-specific-subfield-of-ml">A specific subfield of ML</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reinforcement-learning-examples">Reinforcement Learning examples</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recent-breakthroughs">Recent breakthroughs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#alphago">AlphaGo</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#openai-s-hide-and-seek">OpenAI‚Äôs Hide and Seek</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#terminology">Terminology</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#elements-of-a-rl-system">Elements of a RL system</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#state">State</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#value-functions">Value functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-exploration-vs-exploitation-dilemna">The exploration vs. exploitation dilemna</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-free-vs-model-based-rl">Model-free Vs model-based RL</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-decision-processes">Markov Decision Processes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition">Definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#general-formulation">General formulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finite-markov-decision-processes">Finite Markov Decision Processes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-property">Markov property</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-formulations">Additional formulations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamics-table">Dynamics table</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#modelisation">Modelisation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#return">Return</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#policy">Policy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#state-value-function">State-value function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#action-value-function">Action-value function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimality">Optimality</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tabular-methods">Tabular methods</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#context">Context</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#value-iteration">Value Iteration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#q-value-iteration">Q-Value Iteration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#td-learning">TD Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#q-learning">Q-Learning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#approximate-methods">Approximate methods</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Context</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-gradients">Policy gradients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dqn">DQN</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#actor-critic-methods">Actor-Critic methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a3c">A3C</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ppo">PPO</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="introduction-to-reinforcement-learning">
<h1>Introduction to Reinforcement Learning<a class="headerlink" href="#introduction-to-reinforcement-learning" title="Link to this heading">#</a></h1>
<blockquote>
<div><p>This chapter is inspired by the book <a class="reference external" href="https://github.com/ageron/handson-ml2">Hands-On Machine Learning</a> written by Aur√©lien G√©ron.</p>
</div></blockquote>
<section id="learning-objectives">
<h2>Learning objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Understand what Reinforcement Learning (RL) is about.</p></li>
<li><p>Review Markow Decision Processes.</p></li>
<li><p>Discover some of the main approaches to RL.</p></li>
</ul>
</section>
<section id="environnement-setup">
<h2>Environnement setup<a class="headerlink" href="#environnement-setup" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">platform</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">YouTubeVideo</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup plots</span>

<span class="c1"># Include matplotlib graphs into the notebook, next to the code</span>
<span class="c1"># https://stackoverflow.com/a/43028034/2380880</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="c1"># Improve plot quality</span>
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &quot;retina&quot;

<span class="c1"># Setup seaborn default theme</span>
<span class="c1"># http://seaborn.pydata.org/generated/seaborn.set_theme.html#seaborn.set_theme</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_theme</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print environment info</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Python version: </span><span class="si">{</span><span class="n">platform</span><span class="o">.</span><span class="n">python_version</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;NumPy version: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Python version: 3.11.1
NumPy version: 1.26.3
</pre></div>
</div>
</div>
</div>
</section>
<section id="what-is-reinforcement-learning">
<h2>What is Reinforcement Learning?<a class="headerlink" href="#what-is-reinforcement-learning" title="Link to this heading">#</a></h2>
<section id="rl-in-a-nutshell">
<h3>RL in a nutshell<a class="headerlink" href="#rl-in-a-nutshell" title="Link to this heading">#</a></h3>
<p>Reinforcement Learning is about <strong>learning how to evolve in a dynamic system</strong>, often called the <em>environment</em>.</p>
<p>The learner and decision maker (often called an <em>agent</em>) is not told explicitly which <strong>actions</strong> to take, but instead must discover which actions yield the most <strong>reward</strong> over time by trying them.</p>
<p>Actions may affect not only the immediate reward but also the next situation and, through that, all subsequent rewards.</p>
<p><img alt="Learning to ride a bike!" src="../_images/prosper_bike.gif" /></p>
</section>
<section id="a-specific-subfield-of-ml">
<h3>A specific subfield of ML<a class="headerlink" href="#a-specific-subfield-of-ml" title="Link to this heading">#</a></h3>
<p>RL is different from <strong>supervised learning</strong>, where correct answers (desired behaviour) are given to the learner during training. A RL learner must be able to learn from its own experience.</p>
<p>RL is also different from <strong>unsupervised learning</strong>: finding structure in unlabeled data could help, but does not solve the reward maximisation problem which is at the heart of RL.</p>
<p>Lastly, RL is different from <strong>evolutionary methods</strong>, which only consider the final outcome and ignore the intermediate steps RL is concerned with.</p>
</section>
<section id="reinforcement-learning-examples">
<h3>Reinforcement Learning examples<a class="headerlink" href="#reinforcement-learning-examples" title="Link to this heading">#</a></h3>
<p>RL can be applied to a wide variety of contexts. To name a few:</p>
<ul class="simple">
<li><p>controlling a robot;</p></li>
<li><p>manage a financial portfolio;</p></li>
<li><p>steering a ship;</p></li>
<li><p>playing a game.</p></li>
</ul>
</section>
<section id="recent-breakthroughs">
<h3>Recent breakthroughs<a class="headerlink" href="#recent-breakthroughs" title="Link to this heading">#</a></h3>
<p>RL is not a new field but went mainstream in recent years, mostly due to game-related feats:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://deepmind.com/research/case-studies/alphago-the-story-so-far">AlphaGo</a> beat Go champion Lee Sedol in 2016;</p></li>
<li><p><a class="reference external" href="https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go">AlphaZero</a> achieved superhuman level at chess, shogi and go in less than 24 hours in 2017;</p></li>
<li><p><a class="reference external" href="https://openai.com/blog/openai-five/">OpenAI Five</a> demonstrated expert level play against other competitive Dota 2 teams in 2019;</p></li>
<li><p><a class="reference external" href="https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii">AlphaStar</a> reached StarCraft 2 Grandmaster level (top 0.2% of human players) also in 2019.</p></li>
</ul>
<section id="alphago">
<h4>AlphaGo<a class="headerlink" href="#alphago" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">YouTubeVideo</span><span class="p">(</span><span class="s2">&quot;WXuK6gekU1Y&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/WXuK6gekU1Y"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
</section>
<section id="openai-s-hide-and-seek">
<h4>OpenAI‚Äôs Hide and Seek<a class="headerlink" href="#openai-s-hide-and-seek" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">YouTubeVideo</span><span class="p">(</span><span class="s2">&quot;kopoLzvh5jY&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/kopoLzvh5jY"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
</section>
</section>
</section>
<section id="terminology">
<h2>Terminology<a class="headerlink" href="#terminology" title="Link to this heading">#</a></h2>
<section id="elements-of-a-rl-system">
<h3>Elements of a RL system<a class="headerlink" href="#elements-of-a-rl-system" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>An <strong>agent</strong> that learns what to do.</p></li>
<li><p>The <strong>environment</strong> that characterizes the dynamic system in which the agent evolves.</p></li>
<li><p>The agent‚Äôs <strong>policy</strong> that defines its way of behaving at a given time.</p></li>
<li><p><strong>Reward signals</strong> (or simply rewards) provided by the environment as results of the agent‚Äôs actions.</p></li>
<li><p>Optionally, a <strong>model</strong> of its environment built by the agent and used to predict future evolutions of the dynamic system.</p></li>
</ul>
<p><img alt="Representation of a RL system" src="../_images/rl_agent_env.png" /></p>
</section>
<section id="state">
<h3>State<a class="headerlink" href="#state" title="Link to this heading">#</a></h3>
<p><strong>State</strong> represents the information available to the agent about its environment.</p>
<p>It is used as input to the agent‚Äôs policy and value functions, and (optionally) as both input to and output from the agent‚Äôs model.</p>
<blockquote>
<div><p>The issues of constructing, changing, or learning the state signal are out of the scope of this chapter, which concentrates on the decision-making aspects.</p>
</div></blockquote>
</section>
<section id="value-functions">
<h3>Value functions<a class="headerlink" href="#value-functions" title="Link to this heading">#</a></h3>
<p>Whereas the reward indicates what is good in an immediate sense, a <strong>value function</strong> specifies what is good in the long run.</p>
<p>The value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state.</p>
<p>Action choices should be made based on value judgments, seeking actions that bring about states of highest value, not highest reward.</p>
<p>Unfortunately, unlike rewards that are directly given by the environment, values are generally hard to obtain. They must be <strong>estimated</strong> and re-estimated from the sequences of observations an agent makes over its entire lifetime.</p>
</section>
<section id="the-exploration-vs-exploitation-dilemna">
<h3>The exploration vs. exploitation dilemna<a class="headerlink" href="#the-exploration-vs-exploitation-dilemna" title="Link to this heading">#</a></h3>
<p>The agent has to exploit what it has already experienced, but it also has to explore in order to discover better actions. Neither exploration nor exploitation can be pursued exclusively without failing at the task at hand.</p>
<p><img alt="Exploration VS exploitation" src="../_images/exploration_exploitation.png" /></p>
</section>
<section id="model-free-vs-model-based-rl">
<h3>Model-free Vs model-based RL<a class="headerlink" href="#model-free-vs-model-based-rl" title="Link to this heading">#</a></h3>
<p><strong>Model-free</strong> algorithms learn solely from experience, through trial-and-error.</p>
<p>On the contrary, algorithms that, during learning or acting, exploit predictions of the environment‚Äôs response are said to be <strong>model-based</strong>. This class of algorithms is able to use <em>planning</em>, i.e. taking into account possible future situations before they are actually experienced (<a class="reference external" href="https://ai.stackexchange.com/a/6733">more details</a>).</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The term ‚Äúmodel‚Äù has a different meaning in <a class="reference internal" href="principles.html"><span class="std std-doc">supervised Machine Learning</span></a>.</p>
</div>
</section>
</section>
<section id="markov-decision-processes">
<h2>Markov Decision Processes<a class="headerlink" href="#markov-decision-processes" title="Link to this heading">#</a></h2>
<section id="definition">
<h3>Definition<a class="headerlink" href="#definition" title="Link to this heading">#</a></h3>
<p><strong>Markov Decision Processes</strong> <span id="id1">[<a class="reference internal" href="../reference/bibliography.html#id23" title="RICHARD BELLMAN. A markovian decision process. Journal of Mathematics and Mechanics, 6(5):679‚Äì684, 1957. URL: http://www.jstor.org/stable/24900506 (visited on 2024-02-28).">BELLMAN, 1957</a>]</span> provide a mathematical framework for modeling decision making in discrete-time situations where, through its actions, outcomes are partly under the control of a decision maker.</p>
<p>They are an extension of <strong>Markov chains</strong> in which what happens next depends only on the current state.</p>
<p>MDPs are a classical formalization of sequential decision making.</p>
<p>Many RL problems with discrete actions can be modeled as MDPs.</p>
</section>
<section id="general-formulation">
<h3>General formulation<a class="headerlink" href="#general-formulation" title="Link to this heading">#</a></h3>
<p><img alt="Agent-environment interface" src="../_images/rl_mdp.png" /></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{S}\)</span>: set of all valid states.</p></li>
<li><p><span class="math notranslate nohighlight">\(S_t \in \mathcal{S}\)</span>: observed <strong>state</strong> of the dynamic system at step <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{A}(S_t)\)</span>: set of all valid actions for state <span class="math notranslate nohighlight">\(S_t\)</span> (or simply <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> when the action set is the same in all states).</p></li>
<li><p><span class="math notranslate nohighlight">\(A_t \in \mathcal{A}(S_t)\)</span>: <strong>action</strong> taken by the agent at step <span class="math notranslate nohighlight">\(t\)</span> in order to (try to) control the system.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{R} \subset \mathbb{R}\)</span>: set of all rewards.</p></li>
<li><p><span class="math notranslate nohighlight">\(R_t \in \mathcal{R}\)</span>: numerical <strong>reward</strong> received by the agent at step <span class="math notranslate nohighlight">\(t\)</span>. Both <span class="math notranslate nohighlight">\(S_t\)</span> and <span class="math notranslate nohighlight">\(R_t\)</span> result from the previous action <span class="math notranslate nohighlight">\(A_{t-1}\)</span>.</p></li>
</ul>
</section>
<section id="finite-markov-decision-processes">
<h3>Finite Markov Decision Processes<a class="headerlink" href="#finite-markov-decision-processes" title="Link to this heading">#</a></h3>
<p>In a finite MDP, the states, actions, and rewards spaces (<span class="math notranslate nohighlight">\(\mathcal{S}\)</span>, <span class="math notranslate nohighlight">\(\mathcal{A}\)</span>, and <span class="math notranslate nohighlight">\(\mathcal{R}\)</span>) all have a finite number of elements.</p>
<p><span class="math notranslate nohighlight">\(\forall t \in \mathbb{N}, R_t\)</span> and <span class="math notranslate nohighlight">\(S_t\)</span> are random variables with discrete probability distributions dependent only on <span class="math notranslate nohighlight">\(S_{t-1}\)</span> and <span class="math notranslate nohighlight">\(A_{t-1}\)</span>. The dynamics of a finite MDP are entirely defined by these distributions.</p>
<div class="math notranslate nohighlight">
\[p(s',r|s,a) \equiv P(S_t=s‚Ä≤, R_t=r | S_{t‚àí1}=s, A_{t‚àí1}=a)\]</div>
<div class="math notranslate nohighlight">
\[\forall s \in \mathcal{S}, a \in \mathcal{A}(s), \sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s',r|s,a) = 1\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(s, s' \in \mathcal{S}\)</span>: values of state before and after action <span class="math notranslate nohighlight">\(a\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(a \in \mathcal{A}(s)\)</span>: chosen action.</p></li>
<li><p><span class="math notranslate nohighlight">\(r \in \mathcal{R}\)</span>: value of reward received after action <span class="math notranslate nohighlight">\(a\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(p(s',r|s,a)\)</span>: probability of getting state <span class="math notranslate nohighlight">\(s'\)</span> and reward <span class="math notranslate nohighlight">\(r\)</span> after having selected action <span class="math notranslate nohighlight">\(a\)</span> in state <span class="math notranslate nohighlight">\(s\)</span>.</p></li>
</ul>
</section>
<section id="markov-property">
<h3>Markov property<a class="headerlink" href="#markov-property" title="Link to this heading">#</a></h3>
<p>This <a class="reference external" href="https://en.wikipedia.org/wiki/Markov_property">property</a> refers to processes in which the probability of each possible value for <span class="math notranslate nohighlight">\(S_t\)</span> and <span class="math notranslate nohighlight">\(R_t\)</span> depends only on the immediately preceding state and action, <span class="math notranslate nohighlight">\(S_{t‚àí1}\)</span> and <span class="math notranslate nohighlight">\(A_{t‚àí1}\)</span>, and not at all on earlier states and actions. Said differently: transitions only depend on the most recent state and action, not on prior history.</p>
<p>The state must include information about all aspects of the past agent‚Äìenvironment interactions.</p>
<p>A process with this property is said to be <em>markovian</em>.</p>
</section>
<section id="additional-formulations">
<h3>Additional formulations<a class="headerlink" href="#additional-formulations" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[p(s'|s,a) \equiv P(S_t=s‚Ä≤ | S_{t‚àí1}=s, A_{t‚àí1}=a) = \sum_{r \in \mathcal{R}} p(s',r|s,a)\]</div>
<div class="math notranslate nohighlight">
\[r(s,a) \equiv \mathbb{E} \left[ R_t | S_{t‚àí1}=s, A_{t‚àí1}=a \right] = \sum_{r \in \mathcal{R}} r \sum_{s' \in \mathcal{S}} p(s',r|s,a)\]</div>
<div class="math notranslate nohighlight">
\[r(s,a,s') \equiv \mathbb{E} \left[ R_t | S_{t‚àí1}=s, A_{t‚àí1}=a,  S_t=s' \right] = \sum_{r \in \mathcal{R}} r \frac {p(s',r|s,a)}{p(s'|s,a)}\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(p(s'|s,a)\)</span>: probability of getting state ùë†‚Ä≤ after having selected action <span class="math notranslate nohighlight">\(a\)</span> in state <span class="math notranslate nohighlight">\(s\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(r(s,a)\)</span>: expected reward after having selected <span class="math notranslate nohighlight">\(a\)</span> in <span class="math notranslate nohighlight">\(s\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(r(s,a,s')\)</span>: expected reward after transitioning from <span class="math notranslate nohighlight">\(s\)</span> to <span class="math notranslate nohighlight">\(s'\)</span> through <span class="math notranslate nohighlight">\(a\)</span>.</p></li>
</ul>
</section>
<section id="example">
<h3>Example<a class="headerlink" href="#example" title="Link to this heading">#</a></h3>
<p>The following MDP (<a class="reference external" href="https://github.com/ageron/handson-ml2/blob/master/18_reinforcement_learning.ipynb">source</a>) has three states <span class="math notranslate nohighlight">\(s_0, s_1, s_2\)</span> and up to three possible discrete actions <span class="math notranslate nohighlight">\(a_0,a_1,a_2\)</span> for each state, some of them yielding positive of negative rewards.</p>
<p><img alt="Example of a Markov decision process" src="../_images/mdp_example.png" /></p>
<p>Here, if action <span class="math notranslate nohighlight">\(a_0\)</span> is chosen in state <span class="math notranslate nohighlight">\(s_0\)</span>, then with probability 0.7 we will go to state <span class="math notranslate nohighlight">\(s_0\)</span> with reward +10, with probability 0.3 we will go to state <span class="math notranslate nohighlight">\(s_1\)</span> with no reward, and we cannot go to state <span class="math notranslate nohighlight">\(s_2\)</span>.</p>
<section id="dynamics-table">
<h4>Dynamics table<a class="headerlink" href="#dynamics-table" title="Link to this heading">#</a></h4>
<p>|<span class="math notranslate nohighlight">\(s\)</span>|<span class="math notranslate nohighlight">\(a\)</span>|<span class="math notranslate nohighlight">\(s'\)</span>|<span class="math notranslate nohighlight">\(p(s'|s,a)\)</span>|<span class="math notranslate nohighlight">\(r(s,a,s')\)</span>|
|-|-|-|-|-|
|<span class="math notranslate nohighlight">\(s_0\)</span>|<span class="math notranslate nohighlight">\(a_0\)</span>|<span class="math notranslate nohighlight">\(s_0\)</span>|<span class="math notranslate nohighlight">\(0.7\)</span>|<span class="math notranslate nohighlight">\(+10\)</span>|
|<span class="math notranslate nohighlight">\(s_0\)</span>|<span class="math notranslate nohighlight">\(a_0\)</span>|<span class="math notranslate nohighlight">\(s_1\)</span>|<span class="math notranslate nohighlight">\(0.3\)</span>||
|<span class="math notranslate nohighlight">\(s_0\)</span>|<span class="math notranslate nohighlight">\(a_1\)</span>|<span class="math notranslate nohighlight">\(s_0\)</span>|<span class="math notranslate nohighlight">\(1.0\)</span>||
|<span class="math notranslate nohighlight">\(s_0\)</span>|<span class="math notranslate nohighlight">\(a_2\)</span>|<span class="math notranslate nohighlight">\(s_0\)</span>|<span class="math notranslate nohighlight">\(0.8\)</span>||
|<span class="math notranslate nohighlight">\(s_0\)</span>|<span class="math notranslate nohighlight">\(a_2\)</span>|<span class="math notranslate nohighlight">\(s_1\)</span>|<span class="math notranslate nohighlight">\(0.2\)</span>||
|<span class="math notranslate nohighlight">\(s_1\)</span>|<span class="math notranslate nohighlight">\(a_0\)</span>|<span class="math notranslate nohighlight">\(s_1\)</span>|<span class="math notranslate nohighlight">\(1.0\)</span>||
|<span class="math notranslate nohighlight">\(s_1\)</span>|<span class="math notranslate nohighlight">\(a_2\)</span>|<span class="math notranslate nohighlight">\(s_2\)</span>|<span class="math notranslate nohighlight">\(1.0\)</span>|<span class="math notranslate nohighlight">\(-50\)</span>|
|<span class="math notranslate nohighlight">\(s_2\)</span>|<span class="math notranslate nohighlight">\(a_1\)</span>|<span class="math notranslate nohighlight">\(s_0\)</span>|<span class="math notranslate nohighlight">\(0.8\)</span>|<span class="math notranslate nohighlight">\(+40\)</span>|
|<span class="math notranslate nohighlight">\(s_2\)</span>|<span class="math notranslate nohighlight">\(a_1\)</span>|<span class="math notranslate nohighlight">\(s_1\)</span>|<span class="math notranslate nohighlight">\(0.1\)</span>||
|<span class="math notranslate nohighlight">\(s_2\)</span>|<span class="math notranslate nohighlight">\(a_1\)</span>|<span class="math notranslate nohighlight">\(s_2\)</span>|<span class="math notranslate nohighlight">\(0.1\)</span>||</p>
</section>
<section id="modelisation">
<h4>Modelisation<a class="headerlink" href="#modelisation" title="Link to this heading">#</a></h4>
<p>The previous MDP can be computationally modelised as a set of arrays. States and actions are represented by their 0-based index.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># For each state, store the possible actions</span>
<span class="n">possible_actions</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]]</span>

<span class="c1"># For each state and possible action, store the transition probas p(s&#39;|s,a)</span>
<span class="n">transition_probabilities</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]],</span>
    <span class="p">[[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="kc">None</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]],</span>
    <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="kc">None</span><span class="p">],</span>
<span class="p">]</span>

<span class="c1"># For each state and possible action, store the rewards r(s,a,s&#39;)</span>
<span class="n">rewards</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[[</span><span class="o">+</span><span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span>
    <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">50</span><span class="p">]],</span>
    <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">+</span><span class="mi">40</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="return">
<h3>Return<a class="headerlink" href="#return" title="Link to this heading">#</a></h3>
<p>It is common to evaluate actions based on the sum of all the rewards that came after them, usually applying a <em>discount factor</em> to account for the present value of future rewards.</p>
<div class="math notranslate nohighlight">
\[G_t = R_{t+1} + \gamma R_{t+2} +\gamma^2 R_{t+3} + ¬∑ ¬∑ ¬∑ = \sum\limits_{k=0}^\infty \gamma^k R_{t+k+1} = R_{t+1} + \gamma G_{t+1}\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(G_t\)</span>: sum of discounted rewards received after step <span class="math notranslate nohighlight">\(t\)</span>, called <strong>return</strong>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma \in [0,1]\)</span>: discount factor. A lower value motivates the decision maker to favor taking actions early. Usually, <span class="math notranslate nohighlight">\(0.9&lt;\gamma&lt;0.99\)</span>.</p></li>
</ul>
</section>
<section id="policy">
<h3>Policy<a class="headerlink" href="#policy" title="Link to this heading">#</a></h3>
<p>The algorithm used by the agent to determine its actions is called its <strong>policy</strong>. Policies may be <em>deterministic</em> or <em>stochastic</em> (involving some randomness).</p>
<p>Formally, a policy <span class="math notranslate nohighlight">\(\pi\)</span> is a potentially probabilistic mapping from state space <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> to action space <span class="math notranslate nohighlight">\(\mathcal{A}(s), \forall s \in \mathcal{S}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\pi(a|s) \equiv P(A_t=a|S_t=s)\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\pi(a|s)\)</span>: probability of selecting action <span class="math notranslate nohighlight">\(a\)</span> in state <span class="math notranslate nohighlight">\(s\)</span>.</p></li>
</ul>
</section>
<section id="state-value-function">
<h3>State-value function<a class="headerlink" href="#state-value-function" title="Link to this heading">#</a></h3>
<p>The relationship. between the value function of a state <span class="math notranslate nohighlight">\(v_\pi(s)\)</span> and the value functions of its successor states is called the <em>Bellman equation</em>.</p>
<div class="math notranslate nohighlight">
\[v_\pi(s) = \mathbb{E}_\pi \left[ G_t | S_t=s \right] = \mathbb{E}_\pi\left[ \sum\limits_{k = 0}^\infty \gamma^k R_{t+k+1} \bigg| S_t = s \right] \forall s \in \mathcal{S}\]</div>
<div class="math notranslate nohighlight">
\[v_\pi(s) = \sum_{a \in \mathcal{A}(s)} \pi(a|s) \sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s',r | s,a)\left[r + \gamma v_\pi(s')\right]\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(v_\pi(s)\)</span>: expected return that the agent will get by starting in state <span class="math notranslate nohighlight">\(s\)</span> and following policy <span class="math notranslate nohighlight">\(\pi\)</span>.</p></li>
</ul>
</section>
<section id="action-value-function">
<h3>Action-value function<a class="headerlink" href="#action-value-function" title="Link to this heading">#</a></h3>
<p>The function <span class="math notranslate nohighlight">\(q_\pi(s,a)\)</span> expressing the value of taking a specific action is also called the <em>q-value</em> function.</p>
<div class="math notranslate nohighlight">
\[q_\pi(s,a) = \mathbb{E}_\pi \left[ G_t | S_t=s, A_t=a \right] = \mathbb{E}_\pi\left[ \sum\limits_{k = 0}^\infty \gamma^k R_{t+k+1} \bigg| S_t = s, A_t=a \right]\]</div>
<div class="math notranslate nohighlight">
\[q_\pi(s,a) = \sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s',r | s,a) \left[r + \gamma v_\pi(s')\right]\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(q_\pi(s,a)\)</span>: expected return when starting in state <span class="math notranslate nohighlight">\(s\)</span> and taking  action <span class="math notranslate nohighlight">\(a\)</span>, following policy <span class="math notranslate nohighlight">\(\pi\)</span>.</p></li>
</ul>
</section>
<section id="optimality">
<h3>Optimality<a class="headerlink" href="#optimality" title="Link to this heading">#</a></h3>
<p>The optimal policy <span class="math notranslate nohighlight">\(\pi^*\)</span> is the one that achieves the biggest return over the long run. The <em>Bellman optimality equations</em> defines the values of state and action under an optimal policy.</p>
<div class="math notranslate nohighlight">
\[v_{\pi^*}(s) = v_*(s) = \underset{a \in \mathcal{A}(s)}{max}\;q_*(s,a) = \underset{a}{max} \sum_{s', r}p(s',r | s,a)\left[r + \gamma v_*(s')\right]\]</div>
<div class="math notranslate nohighlight">
\[q_{\pi^*}(s,a) = q_*(s,a) = \sum_{s',r} p(s',r | s,a) \left[r + \gamma \underset{a'}{max} \;q_*(s',a')\right]\]</div>
</section>
</section>
<section id="tabular-methods">
<h2>Tabular methods<a class="headerlink" href="#tabular-methods" title="Link to this heading">#</a></h2>
<section id="context">
<h3>Context<a class="headerlink" href="#context" title="Link to this heading">#</a></h3>
<p>When the number of states and actions is limited, approximate value functions can be represented as arrays (<em>tables</em>) and stored in memory.</p>
<p>In this case, basic algorithms can often find exact solutions, i.e. the optimal value function and the optimal policy.</p>
<p>These <strong>tabular methods</strong> implement the core ideas of RL and form the building blocks of more powerful ones, used when the state and action spaces become too large.</p>
</section>
<section id="value-iteration">
<h3>Value Iteration<a class="headerlink" href="#value-iteration" title="Link to this heading">#</a></h3>
<p>Method for finding the optimal state value for each state.</p>
<ul class="simple">
<li><p>init all state value estimates to zero;</p></li>
<li><p>iteratively update them using the following equation:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[V_{k+1}(s) \leftarrow \underset{a}{max} \sum\limits_{s', r}p(s',r | s,a)\left[r + \gamma V_k(s')\right] = \underset{a}{max}\; Q_k(s,a)\]</div>
<p>Given enough iterations, these estimates will converge to the optimal state values.</p>
</section>
<section id="q-value-iteration">
<h3>Q-Value Iteration<a class="headerlink" href="#q-value-iteration" title="Link to this heading">#</a></h3>
<p>Like the Value iteration algorithm, iteratively compute <span class="math notranslate nohighlight">\(Q_{k+1}(s,a)\)</span> for all <span class="math notranslate nohighlight">\((s,a)\)</span> until convergence.</p>
<div class="math notranslate nohighlight">
\[Q_{k+1}(s,a) \leftarrow \sum\limits_{s',r} p(s',r | s,a) \left[r + \gamma \cdot \underset{a'}{max} \;Q_k(s',a')\right]\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">init_q_values</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Init action-state values to 0 for all possible actions in all states&quot;&quot;&quot;</span>

    <span class="n">q_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>  <span class="c1"># -np.inf for impossible actions</span>
    <span class="k">for</span> <span class="n">state</span><span class="p">,</span> <span class="n">actions</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">possible_actions</span><span class="p">):</span>
        <span class="n">q_values</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">actions</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>  <span class="c1"># for all possible actions</span>
    <span class="k">return</span> <span class="n">q_values</span>


<span class="nb">print</span><span class="p">(</span><span class="n">init_q_values</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[  0.,   0.,   0.],
       [  0., -inf,   0.],
       [-inf,   0., -inf]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">q_value_iteration</span><span class="p">(</span><span class="n">n_iterations</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Implement the Q-Value iteration algorithm&quot;&quot;&quot;</span>

    <span class="n">q_values</span> <span class="o">=</span> <span class="n">init_q_values</span><span class="p">()</span>

    <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>  <span class="c1"># Discount factor - try changing it to 0.95</span>
    <span class="n">n_states</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">possible_actions</span><span class="p">)</span>

    <span class="n">history</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># Store training history for plotting (later)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iterations</span><span class="p">):</span>
        <span class="n">Q_prev</span> <span class="o">=</span> <span class="n">q_values</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Q_prev</span><span class="p">)</span>
        <span class="c1"># Compute Q_k+1 for all states and actions</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_states</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">possible_actions</span><span class="p">[</span><span class="n">s</span><span class="p">]:</span>
                <span class="n">q_values</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">transition_probabilities</span><span class="p">[</span><span class="n">s</span><span class="p">][</span><span class="n">a</span><span class="p">][</span><span class="n">sp</span><span class="p">]</span>
                        <span class="o">*</span> <span class="p">(</span><span class="n">rewards</span><span class="p">[</span><span class="n">s</span><span class="p">][</span><span class="n">a</span><span class="p">][</span><span class="n">sp</span><span class="p">]</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Q_prev</span><span class="p">[</span><span class="n">sp</span><span class="p">]))</span>
                        <span class="k">for</span> <span class="n">sp</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_states</span><span class="p">)</span>
                    <span class="p">]</span>
                <span class="p">)</span>

    <span class="n">history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">q_values</span><span class="p">,</span> <span class="n">history</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_iterations_q_value</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">final_q_values</span><span class="p">,</span> <span class="n">history_q_vi</span> <span class="o">=</span> <span class="n">q_value_iteration</span><span class="p">(</span><span class="n">n_iterations_q_value</span><span class="p">)</span>

<span class="c1"># Show final action-state values</span>
<span class="nb">print</span><span class="p">(</span><span class="n">final_q_values</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[18.91891892 17.02702702 13.62162162]
 [ 0.                -inf -4.87971488]
 [       -inf 50.13365013        -inf]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">print_optimal_actions</span><span class="p">(</span><span class="n">q_values</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Print actions with maximum Q-value for each state&quot;&quot;&quot;</span>

    <span class="c1"># Find action with maximum Q-value for each state</span>
    <span class="n">optimal_actions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">q_values</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">n_states</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">possible_actions</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_states</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Optimal action for state </span><span class="si">{</span><span class="n">s</span><span class="si">}</span><span class="s2"> is a</span><span class="si">{</span><span class="n">optimal_actions</span><span class="p">[</span><span class="n">s</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="n">print_optimal_actions</span><span class="p">(</span><span class="n">final_q_values</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimal action for state 0 is a0
Optimal action for state 1 is a0
Optimal action for state 2 is a1
</pre></div>
</div>
</div>
</div>
</section>
<section id="td-learning">
<h3>TD Learning<a class="headerlink" href="#td-learning" title="Link to this heading">#</a></h3>
<p>When the transition probabilities and rewards are not known in advance, the agent has to experience each state and each transition: once to know the rewards, several times to estimate the probabilities. It must use an <strong>exploration policy</strong> (for example, a purely random one) to traverse the MDP.</p>
<p>As it progresses, the <strong>Temporal Difference (TD) Learning</strong> algorithm updates the estimates of the state values based on the transition and rewards that are actually observed.</p>
<div class="math notranslate nohighlight">
\[V_{k+1}(s) \leftarrow V_k(s) + \alpha\left(r + \gamma V_k(s') - V_k(s)\right) = V_k(s) + \alpha\cdot\delta_k(s, a, s')\]</div>
<div class="math notranslate nohighlight">
\[V_{k+1}(s) \leftarrow (1-\alpha)V_k(s) + \alpha\left(r + \gamma V_k(s')\right)\]</div>
<div class="math notranslate nohighlight">
\[V(s) \underset{\alpha}\leftarrow r + \gamma V(s')\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span>: learning rate, usually small (example: <span class="math notranslate nohighlight">\(0.001\)</span>).</p></li>
<li><p><span class="math notranslate nohighlight">\(\delta_k(s, a, s') = r + \gamma V_k(s') - V_k(s)\)</span>: TD error.</p></li>
</ul>
</section>
<section id="q-learning">
<h3>Q-Learning<a class="headerlink" href="#q-learning" title="Link to this heading">#</a></h3>
<p>Adapted from the Q-Value Iteration algorithm for situations in which transitions and rewards are initially unknown, <strong>Q-Learning</strong> watches the agent play and gradually improves its estimations of the Q-values. Once it has accurate Q-Value estimates (or close enough), then the optimal policy is choosing the action that has the highest Q-Value (i.e. the <em>greedy</em> policy).</p>
<div class="math notranslate nohighlight">
\[Q(s) \underset{\alpha}\leftarrow r + \gamma \cdot \underset{a}{max} \;Q(s',a')\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Perform an action and receive next state and reward&quot;&quot;&quot;</span>

    <span class="n">n_states</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">possible_actions</span><span class="p">)</span>

    <span class="n">probas</span> <span class="o">=</span> <span class="n">transition_probabilities</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span>
    <span class="n">next_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_states</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="n">probas</span><span class="p">)</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">rewards</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">][</span><span class="n">next_state</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span>


<span class="k">def</span> <span class="nf">exploration_policy</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Explore the MDP, returning a random action&quot;&quot;&quot;</span>

    <span class="c1"># This basic exploration policy is sufficient for this simple problem</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">possible_actions</span><span class="p">[</span><span class="n">state</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">q_learning</span><span class="p">(</span><span class="n">n_iterations</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Implement the Q-learning algorithm&quot;&quot;&quot;</span>

    <span class="n">q_values</span> <span class="o">=</span> <span class="n">init_q_values</span><span class="p">()</span>

    <span class="n">alpha0</span> <span class="o">=</span> <span class="mf">0.05</span>  <span class="c1"># initial learning rate</span>
    <span class="n">decay</span> <span class="o">=</span> <span class="mf">0.005</span>  <span class="c1"># learning rate decay</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>  <span class="c1"># discount factor</span>
    <span class="n">state</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># initial state</span>
    <span class="n">history</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># Training history</span>

    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iterations</span><span class="p">):</span>
        <span class="n">history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">q_values</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">exploration_policy</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
        <span class="n">next_q_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">q_values</span><span class="p">[</span><span class="n">next_state</span><span class="p">])</span>  <span class="c1"># greedy policy at the next step</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">iteration</span> <span class="o">*</span> <span class="n">decay</span><span class="p">)</span>  <span class="c1"># learning rate decay</span>
        <span class="n">q_values</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">*=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span>
        <span class="n">q_values</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">next_q_value</span><span class="p">)</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

    <span class="n">history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">q_values</span><span class="p">,</span> <span class="n">history</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Show final action-state values</span>
<span class="n">n_iterations_q_learning</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">final_q_values</span><span class="p">,</span> <span class="n">history_q_learning</span> <span class="o">=</span> <span class="n">q_learning</span><span class="p">(</span><span class="n">n_iterations_q_learning</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">final_q_values</span><span class="p">)</span>

<span class="n">print_optimal_actions</span><span class="p">(</span><span class="n">final_q_values</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[16.94809846 14.48478047 11.37047548]
 [ 0.                -inf -9.84761859]
 [       -inf 47.06994983        -inf]]
Optimal action for state 0 is a0
Optimal action for state 1 is a0
Optimal action for state 2 is a1
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_q_values</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Plot histories for Q-Value iteration and Q-learning algorithms&quot;&quot;&quot;</span>

    <span class="n">final_q_value</span> <span class="o">=</span> <span class="n">history_q_learning</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># final q-value for s0 and a0</span>

    <span class="c1"># Plot training histories for Q-Value Iteration and Q-Learning methods</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Q-Value$(s_0, a_0)$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Q-Value Iteration&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Q-Learning&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">history</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
        <span class="n">axes</span><span class="p">,</span>
        <span class="p">(</span><span class="n">n_iterations_q_value</span><span class="p">,</span> <span class="n">n_iterations_q_learning</span><span class="p">),</span>
        <span class="p">(</span><span class="n">history_q_vi</span><span class="p">,</span> <span class="n">history_q_learning</span><span class="p">),</span>
    <span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">width</span><span class="p">],</span> <span class="p">[</span><span class="n">final_q_value</span><span class="p">,</span> <span class="n">final_q_value</span><span class="p">],</span> <span class="s2">&quot;k--&quot;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">width</span><span class="p">),</span> <span class="n">history</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;b-&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Iterations&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">24</span><span class="p">])</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/eec4eaccec35a14286ff9d54ffd774d7a8b8ca53462f3eb99028cd5d09d5aaf9.png" src="../_images/eec4eaccec35a14286ff9d54ffd774d7a8b8ca53462f3eb99028cd5d09d5aaf9.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_q_values</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="approximate-methods">
<h2>Approximate methods<a class="headerlink" href="#approximate-methods" title="Link to this heading">#</a></h2>
<section id="id2">
<h3>Context<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p>The previous methods become intractable for problems with arbitrarily large state spaces. In such cases, it is hopeless to find an optimal policy or the optimal value function, even in the limit of infinite time and data. The goal instead is to discover a good approximate solution, using functions with a manageable number of parameters.</p>
<p>When dealing with large state spaces, <strong>generalization</strong> (the ability to make sensible decisions based on previous similar encounters) becomes a key issue. Generalization from examples is what <strong>supervised learning</strong> is all about, and many supervized methods have been applied to supplement RL algorithms.</p>
<p>For years, linear combinations of handcrafted features were necessary to estimate value functions through supervised models. Recently, reseachers have started to harness the power of <strong>Deep Learning</strong> for this task, eliminating the need for manual feature engineering.</p>
</section>
<section id="policy-gradients">
<h3>Policy gradients<a class="headerlink" href="#policy-gradients" title="Link to this heading">#</a></h3>
<p>Instead of trying to evaluate actions, <strong>Policy Gradients (PG)</strong> methods learn a parameterized policy that can select actions without consulting a value function. Policy parameters are optimized by following the <em>gradients</em> towards higher rewards.</p>
<p>One popular class of PG algorithms, called REINFORCE algorithms, was <a class="reference external" href="https://homl.info/132">introduced</a> back in 1992.</p>
</section>
<section id="dqn">
<h3>DQN<a class="headerlink" href="#dqn" title="Link to this heading">#</a></h3>
<p><strong>Deep Q-Network (DQN)</strong> was the first RL algorithm to feature a DL model. Introduced in 2014, it was used to learn to play old-school Atari games like Breakout.</p>
<p>DQN runs a deep neural network for approximating Q-Values. The network takes a state s (i.e. the last 4 screenshots of the game) as input, and outputs an estimation of the Q-Values of all actions in that state.</p>
<p><img alt="" src="../_images/dqn.png" /></p>
</section>
<section id="actor-critic-methods">
<h3>Actor-Critic methods<a class="headerlink" href="#actor-critic-methods" title="Link to this heading">#</a></h3>
<p>This family of algorithms combines Policy Gradients with Deep Q-Networks. An Actor-Critic agent contains two neural networks: a policy net and a DQN.</p>
<ul class="simple">
<li><p>the DQN (critic) is trained normally, learning from the agent‚Äôs experiences;</p></li>
<li><p>the policy net (actor) relies on the action values estimated by the DQN, a bit like an athlete learning with the help of a coach. After each action selection, the critic evaluates the new state to determine whether things have gone better or worse than expected.</p></li>
</ul>
</section>
<section id="a3c">
<h3>A3C<a class="headerlink" href="#a3c" title="Link to this heading">#</a></h3>
<p>Introduced by DeepMind researchers in 2016, <strong>Asynchronous Advantage Actor-Critic (A3C)</strong> is an Actor-Critic variant where multiple agents learn in parallel, exploring different copies of the environment. At regular intervals, but asynchronously (hence the name), each agent pushes some weight updates to a master network, then it pulls the latest weights from that network. Each agent thus contributes to improving the master network and benefits from what the other agents have learned.</p>
<p><img alt="" src="../_images/A3C.png" /></p>
</section>
<section id="ppo">
<h3>PPO<a class="headerlink" href="#ppo" title="Link to this heading">#</a></h3>
<p><strong>Proximal Policy Optimization (PPO)</strong> is an algorithm based on Advantage Actor-Critic (A2C). In a nutshell, it makes RL less sensitive to step size without the tradeoffs incurred by other approaches.</p>
<p>In 2019, OpenAI Five, based on the PPO algorithm, defeated the world champions at the multiplayer game Dota 2.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./ml"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="reinforcement_learning.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Reinforcement Learning</p>
      </div>
    </a>
    <a class="right-next"
       href="challenges.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Challenges</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#environnement-setup">Environnement setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-reinforcement-learning">What is Reinforcement Learning?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rl-in-a-nutshell">RL in a nutshell</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-specific-subfield-of-ml">A specific subfield of ML</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reinforcement-learning-examples">Reinforcement Learning examples</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recent-breakthroughs">Recent breakthroughs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#alphago">AlphaGo</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#openai-s-hide-and-seek">OpenAI‚Äôs Hide and Seek</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#terminology">Terminology</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#elements-of-a-rl-system">Elements of a RL system</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#state">State</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#value-functions">Value functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-exploration-vs-exploitation-dilemna">The exploration vs. exploitation dilemna</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-free-vs-model-based-rl">Model-free Vs model-based RL</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-decision-processes">Markov Decision Processes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition">Definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#general-formulation">General formulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finite-markov-decision-processes">Finite Markov Decision Processes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-property">Markov property</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-formulations">Additional formulations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamics-table">Dynamics table</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#modelisation">Modelisation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#return">Return</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#policy">Policy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#state-value-function">State-value function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#action-value-function">Action-value function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimality">Optimality</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tabular-methods">Tabular methods</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#context">Context</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#value-iteration">Value Iteration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#q-value-iteration">Q-Value Iteration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#td-learning">TD Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#q-learning">Q-Learning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#approximate-methods">Approximate methods</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Context</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-gradients">Policy gradients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dqn">DQN</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#actor-critic-methods">Actor-Critic methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a3c">A3C</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ppo">PPO</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Baptiste Pesquet
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2023-present.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>