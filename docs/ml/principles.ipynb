{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45afc3f9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Principles of supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8179726f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703a1f0b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Components of a supervised ML system\n",
    "\n",
    "- Some **data** to learn from.\n",
    "- A **model** to transform data into results.\n",
    "- A **loss function** to quantify how well (or badly) the model is doing.\n",
    "- An **optimization algorithm** to update the model according to the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4997bcec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Features\n",
    "\n",
    "A **feature** is an attribute (property) of the data given to the model: the number of rooms in a house, the color of a pixel in an image, the presence of a specific word in a text, etc. Most of the time, they come under numerical form.\n",
    "\n",
    "A simple ML project might use a single feature, while more sophisticated ones could use millions of them. \n",
    "\n",
    "They are denoted using the $x$ variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a75609f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Label\n",
    "\n",
    "A **label** (or **class** in the context of classification), is a result the model is trying to predict: the future price of an asset, the nature of the animal shown in a picture, the presence or absence of a face, etc.\n",
    "\n",
    "They are denoted using the $y$ variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d6c95c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Samples\n",
    "\n",
    "An **sample**, also called **example**, is a particular instance of data: an individual email, an image, etc.\n",
    "\n",
    "A **labeled sample** includes both its feature(s) and the associated label(s) to predict. An **unlabeled sample** includes only feature(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b95669",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inputs\n",
    "\n",
    "**Inputs** correspond to all features for one sample of the dataset.\n",
    "\n",
    "They are denoted using the $\\pmb{x}$ variable (notice the boldface to indicate that it is a vector).\n",
    "\n",
    "$$\\pmb{x}^{(i)} = \\begin{pmatrix}\n",
    "       \\ x^{(i)}_1 \\\\\n",
    "       \\ x^{(i)}_2 \\\\\n",
    "       \\ \\vdots \\\\\n",
    "       \\ x^{(i)}_n\n",
    "     \\end{pmatrix}$$\n",
    "\n",
    "- $m$: number of samples in the dataset.\n",
    "- $n$: number of features for one sample.\n",
    "- $\\pmb{x}^{(i)}, i \\in [1,m]$: vector of $n$ features.\n",
    "- $x^{(i)}_j, j \\in [1,n]$: value of the $j$th feature for the $i$th data sample.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe237082",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Targets\n",
    "\n",
    "**Targets** are the expected results (labels) associated to a data sample, often called the *ground truth*. They are denoted using the $\\pmb{y}$ variable.\n",
    "\n",
    "Some ML models have to predict more than one value for each sample (for example, in multiclass classification).\n",
    "\n",
    "$$\\pmb{y}^{(i)} = \\begin{pmatrix}\n",
    "       \\ y^{(i)}_1 \\\\\n",
    "       \\ y^{(i)}_2 \\\\\n",
    "       \\ \\vdots \\\\\n",
    "       \\ y^{(i)}_K\n",
    "     \\end{pmatrix}$$\n",
    "\n",
    "- $K$: number of labels associated to a data sample.\n",
    "- $\\pmb{y}^{(i)}, i \\in [1,m]$: vector of $K$ labels.\n",
    "- $y^{(i)}_k, k \\in [1,K]$: value of the $k$th label for the $i$th sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f2297c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inputs matrix\n",
    "\n",
    "Many ML models expect their inputs to come under the form of a $m \\times n$ matrix, often called **design matrix** and denoted $\\pmb{X}$.\n",
    "\n",
    "$$\\pmb{X} = \\begin{bmatrix}\n",
    "       \\ \\pmb{x}^{(1)T} \\\\\n",
    "       \\ \\pmb{x}^{(2)T} \\\\\n",
    "       \\ \\vdots \\\\\n",
    "       \\ \\pmb{x}^{(m)T} \\\\\n",
    "     \\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "       \\ x^{(1)}_1 & x^{(1)}_2 & \\cdots & x^{(1)}_n \\\\\n",
    "       \\ x^{(2)}_1 & x^{(2)}_2 & \\cdots & x^{(2)}_n \\\\\n",
    "       \\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "       \\ x^{(m)}_1 & x^{(m)}_2 & \\cdots & x^{(m)}_n\n",
    "     \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e699f40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Targets matrix\n",
    "\n",
    "Accordingly, expected results are often stored in a $m \\times K$ matrix denoted $\\pmb{Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a7d9c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model\n",
    "\n",
    "The representation learnt from data during training is called a **model**. It defines the relationship between features and labels.\n",
    "\n",
    "Most (but not all) ML systems are model-based.\n",
    "\n",
    "[![Extract from the book Hands-on Machine Learning with Scikit-Learn & TensorFlow by A. GÃ©ron](_images/instance_model_learning.png)](https://github.com/ageron/handson-ml2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b886c3a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The two phases of a model's life\n",
    "\n",
    "- **Training**: using labeled samples, the model learns to find a relationship between features and labels.\n",
    "- **Inference**: the trained model is used to make predictions on unlabeled samples (new data unseen during training)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c6b255",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model parameters and hyperparameters\n",
    "\n",
    "**Parameters**, sometimes called **weights**, are the internal values that affect the computed output of a model. During the training phase, they are algorithmically adjusted for optimal performance w.r.t the loss function. The set of parameters for a model is denoted $\\pmb{\\omega}$ or $\\pmb{\\theta}$.\n",
    "\n",
    "They are not to be confused with **hyperparameters**, which are configuration properties that constrain the model: the maximum depth of a decision tree, the number of layers in a neural networks, etc. Hyperparameters are statically defined before training by the user or by a dedicated tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240e3bc5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hypothesis function\n",
    "\n",
    "Mathamatically speaking, a model is a function of the inputs that depends on its parameters and computes results (which will be compared to targets during the training process).\n",
    "\n",
    "This function, called the **hypothesis function**, is denoted $h_{\\pmb{\\omega}}$. Its output (predicted result) is denoted  $\\pmb{y'}$ or $\\hat{\\pmb{y}}$.\n",
    "\n",
    "$$\\pmb{y'}^{(i)} = h_{\\pmb{\\omega}}(\\pmb{x}^{(i)})$$\n",
    "\n",
    "- $\\pmb{y'}^{(i)}, i \\in [1,m]$: predicted output for the $i$th sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd394e26",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loss function\n",
    "\n",
    "The **loss function**, also called **cost function** or **objective function**, quantifies the difference, often called **error**, between targets (expected results) and actual results computed by the model. Its value at any given time is called the **loss value**, or simply **loss**.\n",
    "\n",
    "By convention, loss functions are usually defined so that lower is better, hence their name. If the model's prediction is perfect, the loss value is zero.\n",
    "\n",
    "The loss function is denoted $\\mathcal{L}$ or $\\mathcal{J}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26155ef1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loss function example\n",
    "\n",
    "The choice of the loss function depends on the problem type. \n",
    "\n",
    "For regression tasks, a popular choice is the **Mean Squared Error** or squared L2 norm.\n",
    "\n",
    "$$\\mathcal{L}_{MSE} = \\frac{1}{m}\\sum_{i=1}^m (\\pmb{y'}^{(i)} - \\pmb{y}^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2065b7d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimization algorithm\n",
    "\n",
    "Used only during the training phase, it aims at finding the set of model parameters (denoted $\\pmb{\\omega^*}$ or $\\pmb{\\theta^*}$) that minimizes the loss value.\n",
    "\n",
    "Depending on the task and the model type, several algorithms of various complexity exist.\n",
    "\n",
    "[![Untrained Vs trained model](_images/LossSideBySide.png)](https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1da773",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be432e20",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab89c95",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Main steps of an ML project\n",
    "\n",
    "1. **Frame** the problem.\n",
    "1. Collect, analyze and prepare **data**.\n",
    "1. Select and train several **models** on data.\n",
    "1. **Tune** the most promising model.\n",
    "1. **Deploy** the model to production and monitor it.\n",
    "\n",
    "[![ML workflow by RedHat](_images/wiidii_ml_workflow.png)](https://www.redhat.com/files/summit/session-assets/2019/T957A0.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649eeb10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Key questions\n",
    "\n",
    "- What is the business objective?\n",
    "- How good are the current solutions?\n",
    "- What data is available?\n",
    "- Is the problem a good fit for ML?\n",
    "- What is the expected learning type (supervised or not, batch/online...)?\n",
    "- How will the model's performance be evaluated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e23327b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Properties of ML-friendly problems\n",
    "\n",
    "- Difficulty to express the actions as rules.\n",
    "- Data too complex for traditional analytical methods.\n",
    "  - High number of features.\n",
    "  - Highly correlated data (data with similar or closely related values).\n",
    "- Performance > interpretability.\n",
    "- Data quality is paramount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9320085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
